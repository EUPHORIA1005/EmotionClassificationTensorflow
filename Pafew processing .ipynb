{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8778ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.signal import medfilt\n",
    "from scipy.signal import wiener\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "genders = ['male', 'female']\n",
    "labels = ['Happy', 'Surprise', 'Disgust', 'Angry', 'Fear', 'Sad', 'Neutral'] # 0-6, 7 classes\n",
    "\n",
    "def preprocess_data(dataPath, train):\n",
    "    if train:\n",
    "        path = os.path.join(dataPath, 'train')\n",
    "        output_dir = os.path.join(dataPath, 'train.csv')\n",
    "    else:\n",
    "        path = os.path.join(dataPath, 'val')\n",
    "        output_dir = os.path.join(dataPath, 'val.csv')\n",
    "    folders = glob.glob(os.path.join(path, '*'))\n",
    "    folders.sort()\n",
    "\n",
    "    with open(output_dir, 'a+') as csv_output_file:\n",
    "        fieldnames = ['User', 'Person_min', 'Max', 'Min', 'Mean', 'Var', 'Mean Abs Diff', 'Mean Abs Second Diff', 'Emotion', 'Gender', 'Age'] # The features extracted\n",
    "        writer = csv.DictWriter(csv_output_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for dir in folders:\n",
    "            with open(os.path.join(dir, 'EDA.csv')) as csv_file:\n",
    "                csv_reader = csv.reader(csv_file)\n",
    "                line_count = 0\n",
    "                data = [] # all data for one person\n",
    "                time_stamp = [] # time stamp for each item\n",
    "\n",
    "                for row in csv_reader:\n",
    "                    if line_count == 0:\n",
    "                        start_time = float(row[0])\n",
    "                    elif line_count == 1:\n",
    "                        freq = float(row[0])\n",
    "                    elif line_count>2 :\n",
    "                        data.append(float(row[0]))\n",
    "                        time_stamp.append(start_time + float((line_count-2)/freq))\n",
    "                    line_count += 1\n",
    "\n",
    "                #person_Max = max(data)\n",
    "                #person_Min = min(data)\n",
    "                data = (data - np.average(data)) / (np.std(data)) # standartization filter\n",
    "                #data = (np.array(data) - float(person_Min)) / (float(person_Max) - float(person_Min)) # normalised data for each person\n",
    "                #data = medfilt(data, 11) # median filter; can be substituted by your preprocessing methods\n",
    "                #data = wiener(data)\n",
    "                #data = savgol_filter(data, 11, 5)\n",
    "                person_Max = max(data)\n",
    "                person_Min = min(data)\n",
    "                \n",
    "            \n",
    "                log = open(os.path.join(dir, 'log.txt'), 'r')\n",
    "                log_count = 0\n",
    "                for line in log:\n",
    "                    if log_count == 0:\n",
    "                        user = line.split(';')[0].split(':')[-1]\n",
    "                        age = line.split(';')[1].split(':')[-1]\n",
    "                        gender = line.split(';')[2].split(':')[-1]\n",
    "                        gender = genders.index(gender.lower())\n",
    "                        log_count += 1\n",
    "                    elif log_count == 1:\n",
    "                        log_count += 1\n",
    "                    else:\n",
    "                        st = float(line.split(';')[1]) # start time of each video\n",
    "                        et = float(line.split(';')[3]) # end time of each video\n",
    "                        video_name = line.split(';')[2]\n",
    "                        if \"_\" in video_name:\n",
    "                            emotion_label = line.split(';')[2].split('.')[0][:-10] # emotion label of each video\n",
    "                        else:\n",
    "                            emotion_label = line.split(';')[2].split('.')[0][:-9]  # emotion label of each video\n",
    "                        emotion_label = labels.index(emotion_label)\n",
    "\n",
    "                        index = np.where(np.logical_and((np.array(time_stamp) >= st), (np.array(time_stamp) <= et)))\n",
    "                        data_list = data[index[0]]\n",
    "                        if len(data_list)== 0:\n",
    "                            break\n",
    "                        diff_list = [data_list[k+1]-data_list[k] for k in range(len(data_list)-1)]\n",
    "                        abs_diff_list = abs(np.array(diff_list))\n",
    "                        second_diff_list = [diff_list[k + 1] - diff_list[k] for k in range(len(diff_list) - 1)]\n",
    "                        abs_second_diff_list = abs(np.array(second_diff_list))\n",
    "                        writer.writerow({'User': user, 'Person_min': person_Min,  'Max': max(data_list), 'Min': min(data_list), 'Mean': np.mean(data_list), 'Var': np.var(data_list), 'Mean Abs Diff': np.mean(abs_diff_list), 'Mean Abs Second Diff': np.mean(abs_second_diff_list),'Emotion': emotion_label, 'Gender': gender, 'Age': age})\n",
    "                log.close()\n",
    "        csv_file.close()\n",
    "    csv_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8285489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and reading dat\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "os.remove('train.csv')\n",
    "preprocess_data('', train=1)\n",
    "os.remove('val.csv')\n",
    "preprocess_data('', train=0)\n",
    "\n",
    "\n",
    "Data_train = pd.read_csv(\"train.csv\", sep = \",\")\n",
    "#Data_train = shuffle(Data_train)\n",
    "#Data_train[Data_train.User == \"Person_25\"].head(10)\n",
    "#Data_train.head(20)\n",
    "\n",
    "Data_val = pd.read_csv(\"val.csv\")\n",
    "#Data_val = shuffle(Data_val)\n",
    "#Data_val[Data_val.User == \"Person_25\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca097b4",
   "metadata": {},
   "source": [
    "# Vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "22e78213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# import seaborn as sns\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(16, 10))\n",
    "# plt.plot(np.arange(0, 1000, 1), Data_train.Mean.iloc[:1000], scaley = 100)\n",
    "# plt.title(\"Mean variations\")\n",
    "# plt.legend([\"y = mean common variation\"])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e9f27eba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #Max\tMin\tMean\tVar\tMean Abs Diff\tMean Abs Second Diff\tEmotion\n",
    "\n",
    "# sns.set(rc = {'figure.figsize':(16, 10)})\n",
    "# sns.set_theme(style=\"darkgrid\")\n",
    "# sns.scatterplot(data = Data_train, x = \"Mean\", y = Data_train.index, hue = \"Emotion\", palette = \"tab10\", x_bins= 150)\n",
    "# #sns.lineplot(data = Data_train.iloc[:1500], x = Data_train.Mean.iloc[:1500], y = np.arange(0, 1500, 1), hue = \"Emotion\", palette = \"tab10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "81e60458",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9a21b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data_train.Emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "909a835f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Person_min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Min</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Var</th>\n",
       "      <th>Mean Abs Diff</th>\n",
       "      <th>Mean Abs Second Diff</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.225095</td>\n",
       "      <td>-0.980049</td>\n",
       "      <td>-0.831819</td>\n",
       "      <td>0.005871</td>\n",
       "      <td>0.063326</td>\n",
       "      <td>0.114377</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.728419</td>\n",
       "      <td>-0.896161</td>\n",
       "      <td>-0.790007</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.049470</td>\n",
       "      <td>0.084974</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.644532</td>\n",
       "      <td>-0.896161</td>\n",
       "      <td>-0.753136</td>\n",
       "      <td>0.002122</td>\n",
       "      <td>0.054144</td>\n",
       "      <td>0.097866</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.644532</td>\n",
       "      <td>-0.938105</td>\n",
       "      <td>-0.729771</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>0.058719</td>\n",
       "      <td>0.107024</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.560645</td>\n",
       "      <td>-0.770363</td>\n",
       "      <td>-0.650419</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.049434</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.476757</td>\n",
       "      <td>-0.728419</td>\n",
       "      <td>-0.576271</td>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.054527</td>\n",
       "      <td>0.099295</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.434814</td>\n",
       "      <td>-0.644532</td>\n",
       "      <td>-0.519835</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.054760</td>\n",
       "      <td>0.097070</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.267039</td>\n",
       "      <td>-0.476757</td>\n",
       "      <td>-0.329043</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.064822</td>\n",
       "      <td>0.103861</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.225095</td>\n",
       "      <td>-0.392870</td>\n",
       "      <td>-0.301751</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.049434</td>\n",
       "      <td>0.083887</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.225095</td>\n",
       "      <td>-0.392870</td>\n",
       "      <td>-0.283275</td>\n",
       "      <td>0.002120</td>\n",
       "      <td>0.057323</td>\n",
       "      <td>0.101243</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.183152</td>\n",
       "      <td>-0.476757</td>\n",
       "      <td>-0.302845</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>0.066061</td>\n",
       "      <td>0.121529</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>0.278196</td>\n",
       "      <td>-0.308983</td>\n",
       "      <td>-0.215626</td>\n",
       "      <td>0.010521</td>\n",
       "      <td>0.083883</td>\n",
       "      <td>0.159090</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.141208</td>\n",
       "      <td>-0.308983</td>\n",
       "      <td>-0.193638</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.047936</td>\n",
       "      <td>0.087588</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.099297</td>\n",
       "      <td>-0.225095</td>\n",
       "      <td>-0.169807</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0.049930</td>\n",
       "      <td>0.079690</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.057353</td>\n",
       "      <td>-0.225095</td>\n",
       "      <td>-0.138676</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.056343</td>\n",
       "      <td>0.101445</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          User  Person_min       Max       Min      Mean       Var  \\\n",
       "984  Person_25   -3.161054 -0.225095 -0.980049 -0.831819  0.005871   \n",
       "985  Person_25   -3.161054 -0.728419 -0.896161 -0.790007  0.001863   \n",
       "986  Person_25   -3.161054 -0.644532 -0.896161 -0.753136  0.002122   \n",
       "987  Person_25   -3.161054 -0.644532 -0.938105 -0.729771  0.003119   \n",
       "988  Person_25   -3.161054 -0.560645 -0.770363 -0.650419  0.002002   \n",
       "989  Person_25   -3.161054 -0.476757 -0.728419 -0.576271  0.003171   \n",
       "990  Person_25   -3.161054 -0.434814 -0.644532 -0.519835  0.002138   \n",
       "991  Person_25   -3.161054 -0.267039 -0.476757 -0.329043  0.003040   \n",
       "992  Person_25   -3.161054 -0.225095 -0.392870 -0.301751  0.001950   \n",
       "993  Person_25   -3.161054 -0.225095 -0.392870 -0.283275  0.002120   \n",
       "994  Person_25   -3.161054 -0.183152 -0.476757 -0.302845  0.003395   \n",
       "995  Person_25   -3.161054  0.278196 -0.308983 -0.215626  0.010521   \n",
       "996  Person_25   -3.161054 -0.141208 -0.308983 -0.193638  0.001600   \n",
       "997  Person_25   -3.161054 -0.099297 -0.225095 -0.169807  0.001341   \n",
       "998  Person_25   -3.161054 -0.057353 -0.225095 -0.138676  0.001805   \n",
       "\n",
       "     Mean Abs Diff  Mean Abs Second Diff  Emotion  Gender  Age  \n",
       "984       0.063326              0.114377        2       0   23  \n",
       "985       0.049470              0.084974        2       0   23  \n",
       "986       0.054144              0.097866        2       0   23  \n",
       "987       0.058719              0.107024        2       0   23  \n",
       "988       0.049434              0.081600        2       0   23  \n",
       "989       0.054527              0.099295        2       0   23  \n",
       "990       0.054760              0.097070        2       0   23  \n",
       "991       0.064822              0.103861        6       0   23  \n",
       "992       0.049434              0.083887        6       0   23  \n",
       "993       0.057323              0.101243        6       0   23  \n",
       "994       0.066061              0.121529        6       0   23  \n",
       "995       0.083883              0.159090        6       0   23  \n",
       "996       0.047936              0.087588        6       0   23  \n",
       "997       0.049930              0.079690        6       0   23  \n",
       "998       0.056343              0.101445        6       0   23  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_train[Data_train.User == \"Person_25\"].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "66ea7f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Person_min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Min</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Var</th>\n",
       "      <th>Mean Abs Diff</th>\n",
       "      <th>Mean Abs Second Diff</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.350926</td>\n",
       "      <td>-0.476757</td>\n",
       "      <td>-0.409106</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.055925</td>\n",
       "      <td>0.095458</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.308983</td>\n",
       "      <td>-0.476757</td>\n",
       "      <td>-0.384915</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.055925</td>\n",
       "      <td>0.089130</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.308983</td>\n",
       "      <td>-0.476757</td>\n",
       "      <td>-0.374634</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.034318</td>\n",
       "      <td>0.063914</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.308983</td>\n",
       "      <td>-0.560645</td>\n",
       "      <td>-0.384804</td>\n",
       "      <td>0.002980</td>\n",
       "      <td>0.058721</td>\n",
       "      <td>0.096121</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>-0.267039</td>\n",
       "      <td>-0.434814</td>\n",
       "      <td>-0.349480</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.061418</td>\n",
       "      <td>0.100976</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>0.823431</td>\n",
       "      <td>0.655689</td>\n",
       "      <td>0.756704</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.045935</td>\n",
       "      <td>0.079690</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>0.823431</td>\n",
       "      <td>0.697600</td>\n",
       "      <td>0.767506</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.039322</td>\n",
       "      <td>0.066298</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>0.865375</td>\n",
       "      <td>0.655689</td>\n",
       "      <td>0.754458</td>\n",
       "      <td>0.002514</td>\n",
       "      <td>0.056240</td>\n",
       "      <td>0.105342</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>0.865375</td>\n",
       "      <td>0.697600</td>\n",
       "      <td>0.783955</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.039322</td>\n",
       "      <td>0.067110</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>0.907318</td>\n",
       "      <td>0.697600</td>\n",
       "      <td>0.813444</td>\n",
       "      <td>0.002497</td>\n",
       "      <td>0.060818</td>\n",
       "      <td>0.105963</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>0.907318</td>\n",
       "      <td>0.739544</td>\n",
       "      <td>0.834870</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.051930</td>\n",
       "      <td>0.096470</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>0.865375</td>\n",
       "      <td>0.697600</td>\n",
       "      <td>0.817439</td>\n",
       "      <td>0.001723</td>\n",
       "      <td>0.052430</td>\n",
       "      <td>0.090510</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>0.907318</td>\n",
       "      <td>0.697600</td>\n",
       "      <td>0.828091</td>\n",
       "      <td>0.002519</td>\n",
       "      <td>0.051813</td>\n",
       "      <td>0.094373</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>1.494497</td>\n",
       "      <td>0.697600</td>\n",
       "      <td>0.842232</td>\n",
       "      <td>0.017541</td>\n",
       "      <td>0.088379</td>\n",
       "      <td>0.167770</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>Person_25</td>\n",
       "      <td>-3.161054</td>\n",
       "      <td>0.907318</td>\n",
       "      <td>0.739544</td>\n",
       "      <td>0.853038</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.123035</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          User  Person_min       Max       Min      Mean       Var  \\\n",
       "672  Person_25   -3.161054 -0.350926 -0.476757 -0.409106  0.001893   \n",
       "673  Person_25   -3.161054 -0.308983 -0.476757 -0.384915  0.001969   \n",
       "674  Person_25   -3.161054 -0.308983 -0.476757 -0.374634  0.001350   \n",
       "675  Person_25   -3.161054 -0.308983 -0.560645 -0.384804  0.002980   \n",
       "676  Person_25   -3.161054 -0.267039 -0.434814 -0.349480  0.002364   \n",
       "677  Person_25   -3.161054  0.823431  0.655689  0.756704  0.001864   \n",
       "678  Person_25   -3.161054  0.823431  0.697600  0.767506  0.001137   \n",
       "679  Person_25   -3.161054  0.865375  0.655689  0.754458  0.002514   \n",
       "680  Person_25   -3.161054  0.865375  0.697600  0.783955  0.001753   \n",
       "681  Person_25   -3.161054  0.907318  0.697600  0.813444  0.002497   \n",
       "682  Person_25   -3.161054  0.907318  0.739544  0.834870  0.001788   \n",
       "683  Person_25   -3.161054  0.865375  0.697600  0.817439  0.001723   \n",
       "684  Person_25   -3.161054  0.907318  0.697600  0.828091  0.002519   \n",
       "685  Person_25   -3.161054  1.494497  0.697600  0.842232  0.017541   \n",
       "686  Person_25   -3.161054  0.907318  0.739544  0.853038  0.002849   \n",
       "\n",
       "     Mean Abs Diff  Mean Abs Second Diff  Emotion  Gender  Age  \n",
       "672       0.055925              0.095458        2       0   23  \n",
       "673       0.055925              0.089130        2       0   23  \n",
       "674       0.034318              0.063914        2       0   23  \n",
       "675       0.058721              0.096121        2       0   23  \n",
       "676       0.061418              0.100976        2       0   23  \n",
       "677       0.045935              0.079690        3       0   23  \n",
       "678       0.039322              0.066298        3       0   23  \n",
       "679       0.056240              0.105342        3       0   23  \n",
       "680       0.039322              0.067110        3       0   23  \n",
       "681       0.060818              0.105963        3       0   23  \n",
       "682       0.051930              0.096470        3       0   23  \n",
       "683       0.052430              0.090510        3       0   23  \n",
       "684       0.051813              0.094373        3       0   23  \n",
       "685       0.088379              0.167770        3       0   23  \n",
       "686       0.068158              0.123035        3       0   23  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_val[Data_val.User == \"Person_25\"].head(15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a95ae",
   "metadata": {},
   "source": [
    "####  Data is distributed normally. No NaN values. Sad and happy emotions have more samples than others -> might have to equalize value counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7072b73",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "08962cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, ReLU\n",
    "from tensorflow.keras.regularizers import L1, L2, L1L2\n",
    "\n",
    "def initModelGRU(shape, outputUnits, outputActivation) -> tf.keras.Model:\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    inputs = tf.keras.Input(shape = (shape))\n",
    "    expand = tf.expand_dims(inputs, axis = 2)\n",
    "                                                            #dropout better than L reg\n",
    "    gru1 = tf.keras.layers.GRU(128, return_sequences = True, recurrent_dropout = 0.25, activation = 'relu')(expand) \n",
    "    gru2 = tf.keras.layers.GRU(64, return_sequences = True, recurrent_dropout = 0.25, activation = 'relu')(gru1)\n",
    "    flatten = tf.keras.layers.Flatten()(gru2)  \n",
    "                                                        #use softmax for prob distribution!\n",
    "    outputs = tf.keras.layers.Dense(outputUnits, activation = outputActivation)(flatten)    \n",
    "    model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def initModelBasic(shape, outputUnits) -> tf.keras.Model:\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(tf.keras.Input(shape = (shape)))\n",
    "    \n",
    "    model.add(Dense(16, activation = 'linear'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "    \n",
    "    model.add(Dense(16, activation = 'linear'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "    \n",
    "    model.add(Dense(outputUnits, activation = 'softmax'))\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9176b7a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                160       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 16)               64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 16)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 119       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 679\n",
      "Trainable params: 615\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/600\n",
      "73/73 [==============================] - 1s 6ms/step - loss: 2.1303 - sparse_categorical_accuracy: 0.1255 - val_loss: 2.0140 - val_sparse_categorical_accuracy: 0.1495\n",
      "Epoch 2/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.9741 - sparse_categorical_accuracy: 0.1558 - val_loss: 1.9756 - val_sparse_categorical_accuracy: 0.1760\n",
      "Epoch 3/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.9349 - sparse_categorical_accuracy: 0.1771 - val_loss: 1.9496 - val_sparse_categorical_accuracy: 0.1640\n",
      "Epoch 4/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.9219 - sparse_categorical_accuracy: 0.2022 - val_loss: 1.9396 - val_sparse_categorical_accuracy: 0.1961\n",
      "Epoch 5/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.9090 - sparse_categorical_accuracy: 0.2074 - val_loss: 1.9303 - val_sparse_categorical_accuracy: 0.1889\n",
      "Epoch 6/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.9045 - sparse_categorical_accuracy: 0.2104 - val_loss: 1.9240 - val_sparse_categorical_accuracy: 0.1961\n",
      "Epoch 7/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8961 - sparse_categorical_accuracy: 0.2152 - val_loss: 1.9260 - val_sparse_categorical_accuracy: 0.1889\n",
      "Epoch 8/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.8900 - sparse_categorical_accuracy: 0.2273 - val_loss: 1.9141 - val_sparse_categorical_accuracy: 0.1897\n",
      "Epoch 9/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8866 - sparse_categorical_accuracy: 0.2333 - val_loss: 1.9168 - val_sparse_categorical_accuracy: 0.1977\n",
      "Epoch 10/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8849 - sparse_categorical_accuracy: 0.2303 - val_loss: 1.9167 - val_sparse_categorical_accuracy: 0.1825\n",
      "Epoch 11/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8826 - sparse_categorical_accuracy: 0.2277 - val_loss: 1.9030 - val_sparse_categorical_accuracy: 0.2146\n",
      "Epoch 12/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8779 - sparse_categorical_accuracy: 0.2338 - val_loss: 1.9201 - val_sparse_categorical_accuracy: 0.1688\n",
      "Epoch 13/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8729 - sparse_categorical_accuracy: 0.2260 - val_loss: 1.9111 - val_sparse_categorical_accuracy: 0.1720\n",
      "Epoch 14/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8726 - sparse_categorical_accuracy: 0.2294 - val_loss: 1.9009 - val_sparse_categorical_accuracy: 0.2066\n",
      "Epoch 15/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8753 - sparse_categorical_accuracy: 0.2333 - val_loss: 1.9000 - val_sparse_categorical_accuracy: 0.2066\n",
      "Epoch 16/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.8709 - sparse_categorical_accuracy: 0.2316 - val_loss: 1.9001 - val_sparse_categorical_accuracy: 0.2106\n",
      "Epoch 17/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.8675 - sparse_categorical_accuracy: 0.2381 - val_loss: 1.9252 - val_sparse_categorical_accuracy: 0.1640\n",
      "Epoch 18/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8583 - sparse_categorical_accuracy: 0.2485 - val_loss: 1.8957 - val_sparse_categorical_accuracy: 0.2146\n",
      "Epoch 19/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8627 - sparse_categorical_accuracy: 0.2398 - val_loss: 1.8915 - val_sparse_categorical_accuracy: 0.2162\n",
      "Epoch 20/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8564 - sparse_categorical_accuracy: 0.2515 - val_loss: 1.9080 - val_sparse_categorical_accuracy: 0.2058\n",
      "Epoch 21/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8572 - sparse_categorical_accuracy: 0.2541 - val_loss: 1.8998 - val_sparse_categorical_accuracy: 0.2275\n",
      "Epoch 22/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.8597 - sparse_categorical_accuracy: 0.2446 - val_loss: 1.8930 - val_sparse_categorical_accuracy: 0.2371\n",
      "Epoch 23/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8484 - sparse_categorical_accuracy: 0.2571 - val_loss: 1.8956 - val_sparse_categorical_accuracy: 0.2291\n",
      "Epoch 24/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8483 - sparse_categorical_accuracy: 0.2589 - val_loss: 1.8823 - val_sparse_categorical_accuracy: 0.2460\n",
      "Epoch 25/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8471 - sparse_categorical_accuracy: 0.2597 - val_loss: 1.8903 - val_sparse_categorical_accuracy: 0.2404\n",
      "Epoch 26/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8479 - sparse_categorical_accuracy: 0.2619 - val_loss: 1.9031 - val_sparse_categorical_accuracy: 0.2042\n",
      "Epoch 27/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8463 - sparse_categorical_accuracy: 0.2658 - val_loss: 1.8845 - val_sparse_categorical_accuracy: 0.2339\n",
      "Epoch 28/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8426 - sparse_categorical_accuracy: 0.2649 - val_loss: 1.8826 - val_sparse_categorical_accuracy: 0.2299\n",
      "Epoch 29/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8460 - sparse_categorical_accuracy: 0.2597 - val_loss: 1.8686 - val_sparse_categorical_accuracy: 0.2379\n",
      "Epoch 30/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8304 - sparse_categorical_accuracy: 0.2606 - val_loss: 1.8709 - val_sparse_categorical_accuracy: 0.2476\n",
      "Epoch 31/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8389 - sparse_categorical_accuracy: 0.2727 - val_loss: 1.8704 - val_sparse_categorical_accuracy: 0.2484\n",
      "Epoch 32/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8307 - sparse_categorical_accuracy: 0.2680 - val_loss: 1.8777 - val_sparse_categorical_accuracy: 0.2444\n",
      "Epoch 33/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8272 - sparse_categorical_accuracy: 0.2645 - val_loss: 1.8943 - val_sparse_categorical_accuracy: 0.2339\n",
      "Epoch 34/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8305 - sparse_categorical_accuracy: 0.2571 - val_loss: 1.8691 - val_sparse_categorical_accuracy: 0.2387\n",
      "Epoch 35/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.8328 - sparse_categorical_accuracy: 0.2706 - val_loss: 1.9131 - val_sparse_categorical_accuracy: 0.2018\n",
      "Epoch 36/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8270 - sparse_categorical_accuracy: 0.2732 - val_loss: 1.9043 - val_sparse_categorical_accuracy: 0.2130\n",
      "Epoch 37/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.8302 - sparse_categorical_accuracy: 0.2641 - val_loss: 1.8703 - val_sparse_categorical_accuracy: 0.2267\n",
      "Epoch 38/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8342 - sparse_categorical_accuracy: 0.2710 - val_loss: 1.8661 - val_sparse_categorical_accuracy: 0.2178\n",
      "Epoch 39/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.8234 - sparse_categorical_accuracy: 0.2662 - val_loss: 1.8568 - val_sparse_categorical_accuracy: 0.2379\n",
      "Epoch 40/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8218 - sparse_categorical_accuracy: 0.2684 - val_loss: 1.8832 - val_sparse_categorical_accuracy: 0.2355\n",
      "Epoch 41/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.8258 - sparse_categorical_accuracy: 0.2567 - val_loss: 1.8728 - val_sparse_categorical_accuracy: 0.2259\n",
      "Epoch 42/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8229 - sparse_categorical_accuracy: 0.2671 - val_loss: 1.8682 - val_sparse_categorical_accuracy: 0.2444\n",
      "Epoch 43/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8133 - sparse_categorical_accuracy: 0.2662 - val_loss: 1.8722 - val_sparse_categorical_accuracy: 0.2291\n",
      "Epoch 44/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8230 - sparse_categorical_accuracy: 0.2697 - val_loss: 1.8748 - val_sparse_categorical_accuracy: 0.2339\n",
      "Epoch 45/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8138 - sparse_categorical_accuracy: 0.2771 - val_loss: 1.8576 - val_sparse_categorical_accuracy: 0.2404\n",
      "Epoch 46/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8103 - sparse_categorical_accuracy: 0.2693 - val_loss: 1.8540 - val_sparse_categorical_accuracy: 0.2516\n",
      "Epoch 47/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8044 - sparse_categorical_accuracy: 0.2788 - val_loss: 1.8672 - val_sparse_categorical_accuracy: 0.2227\n",
      "Epoch 48/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8058 - sparse_categorical_accuracy: 0.2710 - val_loss: 1.8673 - val_sparse_categorical_accuracy: 0.2564\n",
      "Epoch 49/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8106 - sparse_categorical_accuracy: 0.2680 - val_loss: 1.8522 - val_sparse_categorical_accuracy: 0.2677\n",
      "Epoch 50/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8052 - sparse_categorical_accuracy: 0.2701 - val_loss: 1.8644 - val_sparse_categorical_accuracy: 0.2500\n",
      "Epoch 51/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8031 - sparse_categorical_accuracy: 0.2766 - val_loss: 1.8546 - val_sparse_categorical_accuracy: 0.2580\n",
      "Epoch 52/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8073 - sparse_categorical_accuracy: 0.2714 - val_loss: 1.8549 - val_sparse_categorical_accuracy: 0.2339\n",
      "Epoch 53/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8084 - sparse_categorical_accuracy: 0.2801 - val_loss: 1.8467 - val_sparse_categorical_accuracy: 0.2605\n",
      "Epoch 54/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7969 - sparse_categorical_accuracy: 0.2810 - val_loss: 1.8401 - val_sparse_categorical_accuracy: 0.2629\n",
      "Epoch 55/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8054 - sparse_categorical_accuracy: 0.2797 - val_loss: 1.8439 - val_sparse_categorical_accuracy: 0.2508\n",
      "Epoch 56/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8003 - sparse_categorical_accuracy: 0.2732 - val_loss: 1.8613 - val_sparse_categorical_accuracy: 0.2556\n",
      "Epoch 57/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7996 - sparse_categorical_accuracy: 0.2710 - val_loss: 1.8405 - val_sparse_categorical_accuracy: 0.2395\n",
      "Epoch 58/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.8008 - sparse_categorical_accuracy: 0.2810 - val_loss: 1.8549 - val_sparse_categorical_accuracy: 0.2371\n",
      "Epoch 59/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7910 - sparse_categorical_accuracy: 0.2823 - val_loss: 1.8377 - val_sparse_categorical_accuracy: 0.2556\n",
      "Epoch 60/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7972 - sparse_categorical_accuracy: 0.2792 - val_loss: 1.8616 - val_sparse_categorical_accuracy: 0.2307\n",
      "Epoch 61/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7955 - sparse_categorical_accuracy: 0.2853 - val_loss: 1.8480 - val_sparse_categorical_accuracy: 0.2757\n",
      "Epoch 62/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7943 - sparse_categorical_accuracy: 0.2740 - val_loss: 1.8429 - val_sparse_categorical_accuracy: 0.2669\n",
      "Epoch 63/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7837 - sparse_categorical_accuracy: 0.3000 - val_loss: 1.8638 - val_sparse_categorical_accuracy: 0.2580\n",
      "Epoch 64/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7843 - sparse_categorical_accuracy: 0.2736 - val_loss: 1.8534 - val_sparse_categorical_accuracy: 0.2420\n",
      "Epoch 65/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7916 - sparse_categorical_accuracy: 0.2827 - val_loss: 1.8403 - val_sparse_categorical_accuracy: 0.2685\n",
      "Epoch 66/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7862 - sparse_categorical_accuracy: 0.2913 - val_loss: 1.8335 - val_sparse_categorical_accuracy: 0.2773\n",
      "Epoch 67/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7895 - sparse_categorical_accuracy: 0.2844 - val_loss: 1.8374 - val_sparse_categorical_accuracy: 0.2460\n",
      "Epoch 68/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7867 - sparse_categorical_accuracy: 0.2909 - val_loss: 1.8488 - val_sparse_categorical_accuracy: 0.2637\n",
      "Epoch 69/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7875 - sparse_categorical_accuracy: 0.2870 - val_loss: 1.8387 - val_sparse_categorical_accuracy: 0.2315\n",
      "Epoch 70/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7915 - sparse_categorical_accuracy: 0.2814 - val_loss: 1.8343 - val_sparse_categorical_accuracy: 0.2355\n",
      "Epoch 71/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7818 - sparse_categorical_accuracy: 0.2935 - val_loss: 1.8361 - val_sparse_categorical_accuracy: 0.2838\n",
      "Epoch 72/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7824 - sparse_categorical_accuracy: 0.2857 - val_loss: 1.8284 - val_sparse_categorical_accuracy: 0.2436\n",
      "Epoch 73/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7797 - sparse_categorical_accuracy: 0.3035 - val_loss: 1.8439 - val_sparse_categorical_accuracy: 0.2733\n",
      "Epoch 74/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7759 - sparse_categorical_accuracy: 0.2931 - val_loss: 1.8268 - val_sparse_categorical_accuracy: 0.2805\n",
      "Epoch 75/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7797 - sparse_categorical_accuracy: 0.2918 - val_loss: 1.8190 - val_sparse_categorical_accuracy: 0.2709\n",
      "Epoch 76/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7800 - sparse_categorical_accuracy: 0.3039 - val_loss: 1.8165 - val_sparse_categorical_accuracy: 0.2669\n",
      "Epoch 77/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7750 - sparse_categorical_accuracy: 0.2935 - val_loss: 1.8201 - val_sparse_categorical_accuracy: 0.2749\n",
      "Epoch 78/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7870 - sparse_categorical_accuracy: 0.2944 - val_loss: 1.8223 - val_sparse_categorical_accuracy: 0.2773\n",
      "Epoch 79/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7753 - sparse_categorical_accuracy: 0.2861 - val_loss: 1.8776 - val_sparse_categorical_accuracy: 0.2669\n",
      "Epoch 80/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7716 - sparse_categorical_accuracy: 0.2909 - val_loss: 1.8345 - val_sparse_categorical_accuracy: 0.2420\n",
      "Epoch 81/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7762 - sparse_categorical_accuracy: 0.3009 - val_loss: 1.8329 - val_sparse_categorical_accuracy: 0.2428\n",
      "Epoch 82/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7708 - sparse_categorical_accuracy: 0.2974 - val_loss: 1.8367 - val_sparse_categorical_accuracy: 0.2677\n",
      "Epoch 83/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7679 - sparse_categorical_accuracy: 0.2918 - val_loss: 1.8211 - val_sparse_categorical_accuracy: 0.2918\n",
      "Epoch 84/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7699 - sparse_categorical_accuracy: 0.2861 - val_loss: 1.8321 - val_sparse_categorical_accuracy: 0.2420\n",
      "Epoch 85/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7690 - sparse_categorical_accuracy: 0.2866 - val_loss: 1.8339 - val_sparse_categorical_accuracy: 0.2653\n",
      "Epoch 86/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7663 - sparse_categorical_accuracy: 0.2952 - val_loss: 1.8199 - val_sparse_categorical_accuracy: 0.2725\n",
      "Epoch 87/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7737 - sparse_categorical_accuracy: 0.2948 - val_loss: 1.8336 - val_sparse_categorical_accuracy: 0.2596\n",
      "Epoch 88/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.7723 - sparse_categorical_accuracy: 0.2900 - val_loss: 1.8338 - val_sparse_categorical_accuracy: 0.2709\n",
      "Epoch 89/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7634 - sparse_categorical_accuracy: 0.2961 - val_loss: 1.8372 - val_sparse_categorical_accuracy: 0.2621\n",
      "Epoch 90/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7667 - sparse_categorical_accuracy: 0.3000 - val_loss: 1.8667 - val_sparse_categorical_accuracy: 0.2275\n",
      "Epoch 91/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7735 - sparse_categorical_accuracy: 0.2887 - val_loss: 1.8168 - val_sparse_categorical_accuracy: 0.2516\n",
      "Epoch 92/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7683 - sparse_categorical_accuracy: 0.3043 - val_loss: 1.8280 - val_sparse_categorical_accuracy: 0.2645\n",
      "Epoch 93/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7607 - sparse_categorical_accuracy: 0.2965 - val_loss: 1.8339 - val_sparse_categorical_accuracy: 0.2468\n",
      "Epoch 94/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7699 - sparse_categorical_accuracy: 0.2874 - val_loss: 1.8198 - val_sparse_categorical_accuracy: 0.2580\n",
      "Epoch 95/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7676 - sparse_categorical_accuracy: 0.3000 - val_loss: 1.8193 - val_sparse_categorical_accuracy: 0.2701\n",
      "Epoch 96/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7519 - sparse_categorical_accuracy: 0.2978 - val_loss: 1.8346 - val_sparse_categorical_accuracy: 0.2613\n",
      "Epoch 97/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7611 - sparse_categorical_accuracy: 0.2983 - val_loss: 1.8284 - val_sparse_categorical_accuracy: 0.2637\n",
      "Epoch 98/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7628 - sparse_categorical_accuracy: 0.2905 - val_loss: 1.8210 - val_sparse_categorical_accuracy: 0.2709\n",
      "Epoch 99/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7603 - sparse_categorical_accuracy: 0.3009 - val_loss: 1.8535 - val_sparse_categorical_accuracy: 0.2516\n",
      "Epoch 100/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7610 - sparse_categorical_accuracy: 0.3056 - val_loss: 1.8333 - val_sparse_categorical_accuracy: 0.2484\n",
      "Epoch 101/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7569 - sparse_categorical_accuracy: 0.3048 - val_loss: 1.8293 - val_sparse_categorical_accuracy: 0.2629\n",
      "Epoch 102/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7660 - sparse_categorical_accuracy: 0.3017 - val_loss: 1.8271 - val_sparse_categorical_accuracy: 0.2717\n",
      "Epoch 103/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7573 - sparse_categorical_accuracy: 0.2987 - val_loss: 1.8300 - val_sparse_categorical_accuracy: 0.2677\n",
      "Epoch 104/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7616 - sparse_categorical_accuracy: 0.3009 - val_loss: 1.8366 - val_sparse_categorical_accuracy: 0.2765\n",
      "Epoch 105/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7498 - sparse_categorical_accuracy: 0.3078 - val_loss: 1.8272 - val_sparse_categorical_accuracy: 0.2653\n",
      "Epoch 106/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7544 - sparse_categorical_accuracy: 0.3104 - val_loss: 1.8440 - val_sparse_categorical_accuracy: 0.2629\n",
      "Epoch 107/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7547 - sparse_categorical_accuracy: 0.2987 - val_loss: 1.8333 - val_sparse_categorical_accuracy: 0.2629\n",
      "Epoch 108/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7614 - sparse_categorical_accuracy: 0.3000 - val_loss: 1.8161 - val_sparse_categorical_accuracy: 0.2854\n",
      "Epoch 109/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7619 - sparse_categorical_accuracy: 0.3052 - val_loss: 1.8156 - val_sparse_categorical_accuracy: 0.2596\n",
      "Epoch 110/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7545 - sparse_categorical_accuracy: 0.3100 - val_loss: 1.8058 - val_sparse_categorical_accuracy: 0.2685\n",
      "Epoch 111/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7549 - sparse_categorical_accuracy: 0.3069 - val_loss: 1.8467 - val_sparse_categorical_accuracy: 0.2379\n",
      "Epoch 112/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7480 - sparse_categorical_accuracy: 0.3160 - val_loss: 1.8412 - val_sparse_categorical_accuracy: 0.2685\n",
      "Epoch 113/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7442 - sparse_categorical_accuracy: 0.3113 - val_loss: 1.8268 - val_sparse_categorical_accuracy: 0.2484\n",
      "Epoch 114/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7545 - sparse_categorical_accuracy: 0.3065 - val_loss: 1.8151 - val_sparse_categorical_accuracy: 0.2588\n",
      "Epoch 115/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7321 - sparse_categorical_accuracy: 0.3177 - val_loss: 1.8276 - val_sparse_categorical_accuracy: 0.2757\n",
      "Epoch 116/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7594 - sparse_categorical_accuracy: 0.3039 - val_loss: 1.8714 - val_sparse_categorical_accuracy: 0.2500\n",
      "Epoch 117/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7614 - sparse_categorical_accuracy: 0.3087 - val_loss: 1.8679 - val_sparse_categorical_accuracy: 0.2500\n",
      "Epoch 118/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7648 - sparse_categorical_accuracy: 0.2944 - val_loss: 1.8027 - val_sparse_categorical_accuracy: 0.2645\n",
      "Epoch 119/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7527 - sparse_categorical_accuracy: 0.2935 - val_loss: 1.8067 - val_sparse_categorical_accuracy: 0.2613\n",
      "Epoch 120/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7431 - sparse_categorical_accuracy: 0.3121 - val_loss: 1.8343 - val_sparse_categorical_accuracy: 0.2516\n",
      "Epoch 121/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7607 - sparse_categorical_accuracy: 0.2987 - val_loss: 1.7927 - val_sparse_categorical_accuracy: 0.2621\n",
      "Epoch 122/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7483 - sparse_categorical_accuracy: 0.3121 - val_loss: 1.7921 - val_sparse_categorical_accuracy: 0.2789\n",
      "Epoch 123/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7360 - sparse_categorical_accuracy: 0.3182 - val_loss: 1.8024 - val_sparse_categorical_accuracy: 0.2846\n",
      "Epoch 124/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.7410 - sparse_categorical_accuracy: 0.3030 - val_loss: 1.7971 - val_sparse_categorical_accuracy: 0.2773\n",
      "Epoch 125/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7470 - sparse_categorical_accuracy: 0.2974 - val_loss: 1.8288 - val_sparse_categorical_accuracy: 0.2645\n",
      "Epoch 126/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7456 - sparse_categorical_accuracy: 0.3061 - val_loss: 1.7976 - val_sparse_categorical_accuracy: 0.2950\n",
      "Epoch 127/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7415 - sparse_categorical_accuracy: 0.3056 - val_loss: 1.8136 - val_sparse_categorical_accuracy: 0.2516\n",
      "Epoch 128/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7512 - sparse_categorical_accuracy: 0.3108 - val_loss: 1.8037 - val_sparse_categorical_accuracy: 0.2805\n",
      "Epoch 129/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7414 - sparse_categorical_accuracy: 0.3026 - val_loss: 1.8445 - val_sparse_categorical_accuracy: 0.2500\n",
      "Epoch 130/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7534 - sparse_categorical_accuracy: 0.2983 - val_loss: 1.8042 - val_sparse_categorical_accuracy: 0.2765\n",
      "Epoch 131/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7444 - sparse_categorical_accuracy: 0.3169 - val_loss: 1.7993 - val_sparse_categorical_accuracy: 0.2838\n",
      "Epoch 132/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7318 - sparse_categorical_accuracy: 0.3268 - val_loss: 1.8267 - val_sparse_categorical_accuracy: 0.2814\n",
      "Epoch 133/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7416 - sparse_categorical_accuracy: 0.3121 - val_loss: 1.7962 - val_sparse_categorical_accuracy: 0.2725\n",
      "Epoch 134/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7393 - sparse_categorical_accuracy: 0.3212 - val_loss: 1.7891 - val_sparse_categorical_accuracy: 0.2701\n",
      "Epoch 135/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7468 - sparse_categorical_accuracy: 0.3061 - val_loss: 1.7863 - val_sparse_categorical_accuracy: 0.2910\n",
      "Epoch 136/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7420 - sparse_categorical_accuracy: 0.3100 - val_loss: 1.8086 - val_sparse_categorical_accuracy: 0.2677\n",
      "Epoch 137/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7437 - sparse_categorical_accuracy: 0.3156 - val_loss: 1.7995 - val_sparse_categorical_accuracy: 0.2613\n",
      "Epoch 138/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7441 - sparse_categorical_accuracy: 0.3130 - val_loss: 1.8027 - val_sparse_categorical_accuracy: 0.2661\n",
      "Epoch 139/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7421 - sparse_categorical_accuracy: 0.3039 - val_loss: 1.7901 - val_sparse_categorical_accuracy: 0.2846\n",
      "Epoch 140/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7393 - sparse_categorical_accuracy: 0.3160 - val_loss: 1.8374 - val_sparse_categorical_accuracy: 0.2693\n",
      "Epoch 141/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7481 - sparse_categorical_accuracy: 0.3108 - val_loss: 1.8009 - val_sparse_categorical_accuracy: 0.2685\n",
      "Epoch 142/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7408 - sparse_categorical_accuracy: 0.3087 - val_loss: 1.8111 - val_sparse_categorical_accuracy: 0.2661\n",
      "Epoch 143/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7493 - sparse_categorical_accuracy: 0.3013 - val_loss: 1.7990 - val_sparse_categorical_accuracy: 0.2709\n",
      "Epoch 144/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7367 - sparse_categorical_accuracy: 0.3100 - val_loss: 1.7869 - val_sparse_categorical_accuracy: 0.2822\n",
      "Epoch 145/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7317 - sparse_categorical_accuracy: 0.3078 - val_loss: 1.7954 - val_sparse_categorical_accuracy: 0.2757\n",
      "Epoch 146/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7588 - sparse_categorical_accuracy: 0.3082 - val_loss: 1.7970 - val_sparse_categorical_accuracy: 0.2669\n",
      "Epoch 147/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7355 - sparse_categorical_accuracy: 0.3203 - val_loss: 1.7740 - val_sparse_categorical_accuracy: 0.2846\n",
      "Epoch 148/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7374 - sparse_categorical_accuracy: 0.3091 - val_loss: 1.7970 - val_sparse_categorical_accuracy: 0.2733\n",
      "Epoch 149/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7316 - sparse_categorical_accuracy: 0.3061 - val_loss: 1.7878 - val_sparse_categorical_accuracy: 0.2886\n",
      "Epoch 150/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7410 - sparse_categorical_accuracy: 0.3156 - val_loss: 1.7764 - val_sparse_categorical_accuracy: 0.2862\n",
      "Epoch 151/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7624 - sparse_categorical_accuracy: 0.2900 - val_loss: 1.8006 - val_sparse_categorical_accuracy: 0.2733\n",
      "Epoch 152/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7271 - sparse_categorical_accuracy: 0.3130 - val_loss: 1.7859 - val_sparse_categorical_accuracy: 0.2685\n",
      "Epoch 153/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7346 - sparse_categorical_accuracy: 0.3143 - val_loss: 1.8065 - val_sparse_categorical_accuracy: 0.2805\n",
      "Epoch 154/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7415 - sparse_categorical_accuracy: 0.3190 - val_loss: 1.8609 - val_sparse_categorical_accuracy: 0.2645\n",
      "Epoch 155/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7293 - sparse_categorical_accuracy: 0.3113 - val_loss: 1.7886 - val_sparse_categorical_accuracy: 0.2781\n",
      "Epoch 156/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7290 - sparse_categorical_accuracy: 0.3147 - val_loss: 1.8386 - val_sparse_categorical_accuracy: 0.2500\n",
      "Epoch 157/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7294 - sparse_categorical_accuracy: 0.3004 - val_loss: 1.7951 - val_sparse_categorical_accuracy: 0.2717\n",
      "Epoch 158/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7225 - sparse_categorical_accuracy: 0.3126 - val_loss: 1.8213 - val_sparse_categorical_accuracy: 0.2596\n",
      "Epoch 159/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7261 - sparse_categorical_accuracy: 0.3160 - val_loss: 1.7797 - val_sparse_categorical_accuracy: 0.2805\n",
      "Epoch 160/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7388 - sparse_categorical_accuracy: 0.3087 - val_loss: 1.7807 - val_sparse_categorical_accuracy: 0.2918\n",
      "Epoch 161/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7351 - sparse_categorical_accuracy: 0.3173 - val_loss: 1.7875 - val_sparse_categorical_accuracy: 0.2773\n",
      "Epoch 162/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7310 - sparse_categorical_accuracy: 0.3147 - val_loss: 1.7770 - val_sparse_categorical_accuracy: 0.2789\n",
      "Epoch 163/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7241 - sparse_categorical_accuracy: 0.3199 - val_loss: 1.7941 - val_sparse_categorical_accuracy: 0.2709\n",
      "Epoch 164/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7337 - sparse_categorical_accuracy: 0.3156 - val_loss: 1.7977 - val_sparse_categorical_accuracy: 0.2781\n",
      "Epoch 165/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7301 - sparse_categorical_accuracy: 0.3117 - val_loss: 1.8150 - val_sparse_categorical_accuracy: 0.2717\n",
      "Epoch 166/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7488 - sparse_categorical_accuracy: 0.3009 - val_loss: 1.7754 - val_sparse_categorical_accuracy: 0.2918\n",
      "Epoch 167/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.7329 - sparse_categorical_accuracy: 0.3100 - val_loss: 1.7942 - val_sparse_categorical_accuracy: 0.2910\n",
      "Epoch 168/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7258 - sparse_categorical_accuracy: 0.3100 - val_loss: 1.7772 - val_sparse_categorical_accuracy: 0.2950\n",
      "Epoch 169/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7443 - sparse_categorical_accuracy: 0.3143 - val_loss: 1.7731 - val_sparse_categorical_accuracy: 0.2894\n",
      "Epoch 170/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7371 - sparse_categorical_accuracy: 0.3035 - val_loss: 1.7971 - val_sparse_categorical_accuracy: 0.2629\n",
      "Epoch 171/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7227 - sparse_categorical_accuracy: 0.3143 - val_loss: 1.7788 - val_sparse_categorical_accuracy: 0.2910\n",
      "Epoch 172/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7225 - sparse_categorical_accuracy: 0.3152 - val_loss: 1.7705 - val_sparse_categorical_accuracy: 0.2958\n",
      "Epoch 173/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7317 - sparse_categorical_accuracy: 0.3130 - val_loss: 1.7831 - val_sparse_categorical_accuracy: 0.2725\n",
      "Epoch 174/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7335 - sparse_categorical_accuracy: 0.3147 - val_loss: 1.7764 - val_sparse_categorical_accuracy: 0.2998\n",
      "Epoch 175/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7156 - sparse_categorical_accuracy: 0.3203 - val_loss: 1.7893 - val_sparse_categorical_accuracy: 0.2870\n",
      "Epoch 176/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7258 - sparse_categorical_accuracy: 0.3139 - val_loss: 1.7930 - val_sparse_categorical_accuracy: 0.2886\n",
      "Epoch 177/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7221 - sparse_categorical_accuracy: 0.3225 - val_loss: 1.8175 - val_sparse_categorical_accuracy: 0.2572\n",
      "Epoch 178/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7213 - sparse_categorical_accuracy: 0.3251 - val_loss: 1.8032 - val_sparse_categorical_accuracy: 0.2685\n",
      "Epoch 179/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7221 - sparse_categorical_accuracy: 0.3061 - val_loss: 1.8129 - val_sparse_categorical_accuracy: 0.2725\n",
      "Epoch 180/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7438 - sparse_categorical_accuracy: 0.2983 - val_loss: 1.7888 - val_sparse_categorical_accuracy: 0.2781\n",
      "Epoch 181/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7266 - sparse_categorical_accuracy: 0.3199 - val_loss: 1.8072 - val_sparse_categorical_accuracy: 0.2757\n",
      "Epoch 182/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7262 - sparse_categorical_accuracy: 0.3169 - val_loss: 1.7690 - val_sparse_categorical_accuracy: 0.2982\n",
      "Epoch 183/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7303 - sparse_categorical_accuracy: 0.3199 - val_loss: 1.7745 - val_sparse_categorical_accuracy: 0.2886\n",
      "Epoch 184/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7213 - sparse_categorical_accuracy: 0.3165 - val_loss: 1.7840 - val_sparse_categorical_accuracy: 0.2854\n",
      "Epoch 185/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7101 - sparse_categorical_accuracy: 0.3307 - val_loss: 1.7742 - val_sparse_categorical_accuracy: 0.2942\n",
      "Epoch 186/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7319 - sparse_categorical_accuracy: 0.3091 - val_loss: 1.8198 - val_sparse_categorical_accuracy: 0.2685\n",
      "Epoch 187/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7166 - sparse_categorical_accuracy: 0.3273 - val_loss: 1.7702 - val_sparse_categorical_accuracy: 0.2958\n",
      "Epoch 188/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7274 - sparse_categorical_accuracy: 0.3156 - val_loss: 1.7750 - val_sparse_categorical_accuracy: 0.3143\n",
      "Epoch 189/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7219 - sparse_categorical_accuracy: 0.3147 - val_loss: 1.7657 - val_sparse_categorical_accuracy: 0.3031\n",
      "Epoch 190/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7032 - sparse_categorical_accuracy: 0.3372 - val_loss: 1.7864 - val_sparse_categorical_accuracy: 0.2797\n",
      "Epoch 191/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7304 - sparse_categorical_accuracy: 0.3078 - val_loss: 1.8002 - val_sparse_categorical_accuracy: 0.2797\n",
      "Epoch 192/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7091 - sparse_categorical_accuracy: 0.3182 - val_loss: 1.7781 - val_sparse_categorical_accuracy: 0.2966\n",
      "Epoch 193/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7207 - sparse_categorical_accuracy: 0.3108 - val_loss: 1.7852 - val_sparse_categorical_accuracy: 0.2958\n",
      "Epoch 194/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7295 - sparse_categorical_accuracy: 0.3152 - val_loss: 1.8281 - val_sparse_categorical_accuracy: 0.2605\n",
      "Epoch 195/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7277 - sparse_categorical_accuracy: 0.3212 - val_loss: 1.8019 - val_sparse_categorical_accuracy: 0.2822\n",
      "Epoch 196/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7108 - sparse_categorical_accuracy: 0.3281 - val_loss: 1.7732 - val_sparse_categorical_accuracy: 0.3047\n",
      "Epoch 197/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7294 - sparse_categorical_accuracy: 0.3113 - val_loss: 1.7907 - val_sparse_categorical_accuracy: 0.2797\n",
      "Epoch 198/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7384 - sparse_categorical_accuracy: 0.3004 - val_loss: 1.7761 - val_sparse_categorical_accuracy: 0.3063\n",
      "Epoch 199/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7154 - sparse_categorical_accuracy: 0.3238 - val_loss: 1.7980 - val_sparse_categorical_accuracy: 0.2894\n",
      "Epoch 200/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7270 - sparse_categorical_accuracy: 0.3147 - val_loss: 1.7708 - val_sparse_categorical_accuracy: 0.3006\n",
      "Epoch 201/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7288 - sparse_categorical_accuracy: 0.3182 - val_loss: 1.8226 - val_sparse_categorical_accuracy: 0.2717\n",
      "Epoch 202/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7131 - sparse_categorical_accuracy: 0.3126 - val_loss: 1.8029 - val_sparse_categorical_accuracy: 0.2838\n",
      "Epoch 203/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7269 - sparse_categorical_accuracy: 0.3126 - val_loss: 1.7680 - val_sparse_categorical_accuracy: 0.2950\n",
      "Epoch 204/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7262 - sparse_categorical_accuracy: 0.3126 - val_loss: 1.7746 - val_sparse_categorical_accuracy: 0.3014\n",
      "Epoch 205/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7411 - sparse_categorical_accuracy: 0.3152 - val_loss: 1.8207 - val_sparse_categorical_accuracy: 0.2605\n",
      "Epoch 206/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7028 - sparse_categorical_accuracy: 0.3091 - val_loss: 1.7916 - val_sparse_categorical_accuracy: 0.2661\n",
      "Epoch 207/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7340 - sparse_categorical_accuracy: 0.3152 - val_loss: 1.8034 - val_sparse_categorical_accuracy: 0.2653\n",
      "Epoch 208/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7150 - sparse_categorical_accuracy: 0.3221 - val_loss: 1.7626 - val_sparse_categorical_accuracy: 0.2838\n",
      "Epoch 209/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7108 - sparse_categorical_accuracy: 0.3238 - val_loss: 1.7927 - val_sparse_categorical_accuracy: 0.2854\n",
      "Epoch 210/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7076 - sparse_categorical_accuracy: 0.3312 - val_loss: 1.7745 - val_sparse_categorical_accuracy: 0.2822\n",
      "Epoch 211/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7172 - sparse_categorical_accuracy: 0.3048 - val_loss: 1.7742 - val_sparse_categorical_accuracy: 0.2942\n",
      "Epoch 212/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7104 - sparse_categorical_accuracy: 0.3177 - val_loss: 1.7980 - val_sparse_categorical_accuracy: 0.2950\n",
      "Epoch 213/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7112 - sparse_categorical_accuracy: 0.3190 - val_loss: 1.8045 - val_sparse_categorical_accuracy: 0.2781\n",
      "Epoch 214/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7193 - sparse_categorical_accuracy: 0.3190 - val_loss: 1.7956 - val_sparse_categorical_accuracy: 0.2701\n",
      "Epoch 215/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7068 - sparse_categorical_accuracy: 0.3221 - val_loss: 1.7835 - val_sparse_categorical_accuracy: 0.2942\n",
      "Epoch 216/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7091 - sparse_categorical_accuracy: 0.3147 - val_loss: 1.7967 - val_sparse_categorical_accuracy: 0.2741\n",
      "Epoch 217/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7203 - sparse_categorical_accuracy: 0.3152 - val_loss: 1.7700 - val_sparse_categorical_accuracy: 0.2934\n",
      "Epoch 218/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7149 - sparse_categorical_accuracy: 0.3152 - val_loss: 1.7691 - val_sparse_categorical_accuracy: 0.3055\n",
      "Epoch 219/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7113 - sparse_categorical_accuracy: 0.3260 - val_loss: 1.7786 - val_sparse_categorical_accuracy: 0.2822\n",
      "Epoch 220/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7152 - sparse_categorical_accuracy: 0.3160 - val_loss: 1.7739 - val_sparse_categorical_accuracy: 0.2974\n",
      "Epoch 221/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7175 - sparse_categorical_accuracy: 0.3190 - val_loss: 1.7735 - val_sparse_categorical_accuracy: 0.2918\n",
      "Epoch 222/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7147 - sparse_categorical_accuracy: 0.3325 - val_loss: 1.8095 - val_sparse_categorical_accuracy: 0.2701\n",
      "Epoch 223/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7145 - sparse_categorical_accuracy: 0.3108 - val_loss: 1.7726 - val_sparse_categorical_accuracy: 0.2749\n",
      "Epoch 224/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7088 - sparse_categorical_accuracy: 0.3355 - val_loss: 1.7634 - val_sparse_categorical_accuracy: 0.2950\n",
      "Epoch 225/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7019 - sparse_categorical_accuracy: 0.3238 - val_loss: 1.7676 - val_sparse_categorical_accuracy: 0.2878\n",
      "Epoch 226/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6956 - sparse_categorical_accuracy: 0.3299 - val_loss: 1.7581 - val_sparse_categorical_accuracy: 0.3039\n",
      "Epoch 227/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7176 - sparse_categorical_accuracy: 0.3169 - val_loss: 1.7646 - val_sparse_categorical_accuracy: 0.2990\n",
      "Epoch 228/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.7082 - sparse_categorical_accuracy: 0.3264 - val_loss: 1.7656 - val_sparse_categorical_accuracy: 0.2966\n",
      "Epoch 229/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7080 - sparse_categorical_accuracy: 0.3242 - val_loss: 1.7816 - val_sparse_categorical_accuracy: 0.3240\n",
      "Epoch 230/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7110 - sparse_categorical_accuracy: 0.3203 - val_loss: 1.7915 - val_sparse_categorical_accuracy: 0.2733\n",
      "Epoch 231/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6987 - sparse_categorical_accuracy: 0.3372 - val_loss: 1.7722 - val_sparse_categorical_accuracy: 0.2918\n",
      "Epoch 232/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7150 - sparse_categorical_accuracy: 0.3199 - val_loss: 1.7502 - val_sparse_categorical_accuracy: 0.3039\n",
      "Epoch 233/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7039 - sparse_categorical_accuracy: 0.3234 - val_loss: 1.7573 - val_sparse_categorical_accuracy: 0.3127\n",
      "Epoch 234/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7123 - sparse_categorical_accuracy: 0.3195 - val_loss: 1.7692 - val_sparse_categorical_accuracy: 0.2862\n",
      "Epoch 235/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7202 - sparse_categorical_accuracy: 0.3264 - val_loss: 1.7627 - val_sparse_categorical_accuracy: 0.2886\n",
      "Epoch 236/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7121 - sparse_categorical_accuracy: 0.3199 - val_loss: 1.8020 - val_sparse_categorical_accuracy: 0.3071\n",
      "Epoch 237/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6931 - sparse_categorical_accuracy: 0.3268 - val_loss: 1.7934 - val_sparse_categorical_accuracy: 0.2781\n",
      "Epoch 238/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6978 - sparse_categorical_accuracy: 0.3359 - val_loss: 1.7565 - val_sparse_categorical_accuracy: 0.3111\n",
      "Epoch 239/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7121 - sparse_categorical_accuracy: 0.3268 - val_loss: 1.7404 - val_sparse_categorical_accuracy: 0.3215\n",
      "Epoch 240/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7133 - sparse_categorical_accuracy: 0.3095 - val_loss: 1.7636 - val_sparse_categorical_accuracy: 0.3071\n",
      "Epoch 241/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7084 - sparse_categorical_accuracy: 0.3312 - val_loss: 1.7740 - val_sparse_categorical_accuracy: 0.2974\n",
      "Epoch 242/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7130 - sparse_categorical_accuracy: 0.3294 - val_loss: 1.7499 - val_sparse_categorical_accuracy: 0.3014\n",
      "Epoch 243/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7112 - sparse_categorical_accuracy: 0.3351 - val_loss: 1.7621 - val_sparse_categorical_accuracy: 0.3006\n",
      "Epoch 244/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7160 - sparse_categorical_accuracy: 0.3195 - val_loss: 1.7429 - val_sparse_categorical_accuracy: 0.3199\n",
      "Epoch 245/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7015 - sparse_categorical_accuracy: 0.3281 - val_loss: 1.7752 - val_sparse_categorical_accuracy: 0.2934\n",
      "Epoch 246/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7038 - sparse_categorical_accuracy: 0.3325 - val_loss: 1.7636 - val_sparse_categorical_accuracy: 0.2974\n",
      "Epoch 247/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6974 - sparse_categorical_accuracy: 0.3307 - val_loss: 1.7498 - val_sparse_categorical_accuracy: 0.3039\n",
      "Epoch 248/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7229 - sparse_categorical_accuracy: 0.3160 - val_loss: 1.7474 - val_sparse_categorical_accuracy: 0.3079\n",
      "Epoch 249/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7110 - sparse_categorical_accuracy: 0.3299 - val_loss: 1.7747 - val_sparse_categorical_accuracy: 0.2870\n",
      "Epoch 250/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6926 - sparse_categorical_accuracy: 0.3368 - val_loss: 1.7702 - val_sparse_categorical_accuracy: 0.3063\n",
      "Epoch 251/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7010 - sparse_categorical_accuracy: 0.3234 - val_loss: 1.7766 - val_sparse_categorical_accuracy: 0.2926\n",
      "Epoch 252/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7084 - sparse_categorical_accuracy: 0.3281 - val_loss: 1.7439 - val_sparse_categorical_accuracy: 0.3127\n",
      "Epoch 253/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7052 - sparse_categorical_accuracy: 0.3190 - val_loss: 1.7984 - val_sparse_categorical_accuracy: 0.2926\n",
      "Epoch 254/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7030 - sparse_categorical_accuracy: 0.3277 - val_loss: 1.7563 - val_sparse_categorical_accuracy: 0.3006\n",
      "Epoch 255/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7025 - sparse_categorical_accuracy: 0.3355 - val_loss: 1.7717 - val_sparse_categorical_accuracy: 0.3159\n",
      "Epoch 256/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7080 - sparse_categorical_accuracy: 0.3234 - val_loss: 1.7503 - val_sparse_categorical_accuracy: 0.2918\n",
      "Epoch 257/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7026 - sparse_categorical_accuracy: 0.3316 - val_loss: 1.7482 - val_sparse_categorical_accuracy: 0.3127\n",
      "Epoch 258/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7028 - sparse_categorical_accuracy: 0.3264 - val_loss: 1.7686 - val_sparse_categorical_accuracy: 0.3047\n",
      "Epoch 259/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7077 - sparse_categorical_accuracy: 0.3234 - val_loss: 1.7718 - val_sparse_categorical_accuracy: 0.3047\n",
      "Epoch 260/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6954 - sparse_categorical_accuracy: 0.3294 - val_loss: 1.7417 - val_sparse_categorical_accuracy: 0.3167\n",
      "Epoch 261/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7066 - sparse_categorical_accuracy: 0.3165 - val_loss: 1.7826 - val_sparse_categorical_accuracy: 0.3031\n",
      "Epoch 262/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7072 - sparse_categorical_accuracy: 0.3264 - val_loss: 1.7752 - val_sparse_categorical_accuracy: 0.3071\n",
      "Epoch 263/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7011 - sparse_categorical_accuracy: 0.3385 - val_loss: 1.7655 - val_sparse_categorical_accuracy: 0.3039\n",
      "Epoch 264/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6993 - sparse_categorical_accuracy: 0.3316 - val_loss: 1.7581 - val_sparse_categorical_accuracy: 0.3215\n",
      "Epoch 265/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7210 - sparse_categorical_accuracy: 0.3286 - val_loss: 1.7408 - val_sparse_categorical_accuracy: 0.3167\n",
      "Epoch 266/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6905 - sparse_categorical_accuracy: 0.3320 - val_loss: 1.7774 - val_sparse_categorical_accuracy: 0.2870\n",
      "Epoch 267/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7183 - sparse_categorical_accuracy: 0.3229 - val_loss: 1.8349 - val_sparse_categorical_accuracy: 0.2741\n",
      "Epoch 268/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6893 - sparse_categorical_accuracy: 0.3264 - val_loss: 1.7560 - val_sparse_categorical_accuracy: 0.3095\n",
      "Epoch 269/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6984 - sparse_categorical_accuracy: 0.3299 - val_loss: 1.7761 - val_sparse_categorical_accuracy: 0.2757\n",
      "Epoch 270/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6994 - sparse_categorical_accuracy: 0.3147 - val_loss: 1.7935 - val_sparse_categorical_accuracy: 0.2781\n",
      "Epoch 271/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6972 - sparse_categorical_accuracy: 0.3290 - val_loss: 1.7802 - val_sparse_categorical_accuracy: 0.3014\n",
      "Epoch 272/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.7054 - sparse_categorical_accuracy: 0.3299 - val_loss: 1.7442 - val_sparse_categorical_accuracy: 0.3039\n",
      "Epoch 273/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.7037 - sparse_categorical_accuracy: 0.3225 - val_loss: 1.7652 - val_sparse_categorical_accuracy: 0.2886\n",
      "Epoch 274/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.7081 - sparse_categorical_accuracy: 0.3312 - val_loss: 1.7372 - val_sparse_categorical_accuracy: 0.3312\n",
      "Epoch 275/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6917 - sparse_categorical_accuracy: 0.3511 - val_loss: 1.7603 - val_sparse_categorical_accuracy: 0.3223\n",
      "Epoch 276/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.6945 - sparse_categorical_accuracy: 0.3390 - val_loss: 1.7589 - val_sparse_categorical_accuracy: 0.2966\n",
      "Epoch 277/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7190 - sparse_categorical_accuracy: 0.3368 - val_loss: 1.7588 - val_sparse_categorical_accuracy: 0.2998\n",
      "Epoch 278/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.7091 - sparse_categorical_accuracy: 0.3355 - val_loss: 1.7610 - val_sparse_categorical_accuracy: 0.3023\n",
      "Epoch 279/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6973 - sparse_categorical_accuracy: 0.3325 - val_loss: 1.7582 - val_sparse_categorical_accuracy: 0.2918\n",
      "Epoch 280/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7053 - sparse_categorical_accuracy: 0.3255 - val_loss: 1.7428 - val_sparse_categorical_accuracy: 0.3199\n",
      "Epoch 281/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6953 - sparse_categorical_accuracy: 0.3455 - val_loss: 1.7410 - val_sparse_categorical_accuracy: 0.3175\n",
      "Epoch 282/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7051 - sparse_categorical_accuracy: 0.3333 - val_loss: 1.7587 - val_sparse_categorical_accuracy: 0.2797\n",
      "Epoch 283/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7030 - sparse_categorical_accuracy: 0.3229 - val_loss: 1.7704 - val_sparse_categorical_accuracy: 0.2934\n",
      "Epoch 284/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7016 - sparse_categorical_accuracy: 0.3372 - val_loss: 1.7347 - val_sparse_categorical_accuracy: 0.3119\n",
      "Epoch 285/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6587 - sparse_categorical_accuracy: 0.3550 - val_loss: 1.7266 - val_sparse_categorical_accuracy: 0.3223\n",
      "Epoch 286/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6987 - sparse_categorical_accuracy: 0.3299 - val_loss: 1.7460 - val_sparse_categorical_accuracy: 0.3191\n",
      "Epoch 287/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6894 - sparse_categorical_accuracy: 0.3273 - val_loss: 1.8037 - val_sparse_categorical_accuracy: 0.2838\n",
      "Epoch 288/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6909 - sparse_categorical_accuracy: 0.3307 - val_loss: 1.7435 - val_sparse_categorical_accuracy: 0.2958\n",
      "Epoch 289/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6958 - sparse_categorical_accuracy: 0.3403 - val_loss: 1.8182 - val_sparse_categorical_accuracy: 0.2854\n",
      "Epoch 290/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7038 - sparse_categorical_accuracy: 0.3247 - val_loss: 1.7785 - val_sparse_categorical_accuracy: 0.2773\n",
      "Epoch 291/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7160 - sparse_categorical_accuracy: 0.3260 - val_loss: 1.7352 - val_sparse_categorical_accuracy: 0.3095\n",
      "Epoch 292/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7052 - sparse_categorical_accuracy: 0.3169 - val_loss: 1.7510 - val_sparse_categorical_accuracy: 0.3087\n",
      "Epoch 293/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6940 - sparse_categorical_accuracy: 0.3325 - val_loss: 1.7480 - val_sparse_categorical_accuracy: 0.2966\n",
      "Epoch 294/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6820 - sparse_categorical_accuracy: 0.3459 - val_loss: 1.7539 - val_sparse_categorical_accuracy: 0.2974\n",
      "Epoch 295/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6988 - sparse_categorical_accuracy: 0.3307 - val_loss: 1.7560 - val_sparse_categorical_accuracy: 0.2942\n",
      "Epoch 296/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6987 - sparse_categorical_accuracy: 0.3377 - val_loss: 1.7642 - val_sparse_categorical_accuracy: 0.3006\n",
      "Epoch 297/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6846 - sparse_categorical_accuracy: 0.3381 - val_loss: 1.7646 - val_sparse_categorical_accuracy: 0.3095\n",
      "Epoch 298/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7040 - sparse_categorical_accuracy: 0.3169 - val_loss: 1.7567 - val_sparse_categorical_accuracy: 0.2966\n",
      "Epoch 299/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6991 - sparse_categorical_accuracy: 0.3221 - val_loss: 1.7465 - val_sparse_categorical_accuracy: 0.3006\n",
      "Epoch 300/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7057 - sparse_categorical_accuracy: 0.3268 - val_loss: 1.7592 - val_sparse_categorical_accuracy: 0.2982\n",
      "Epoch 301/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6979 - sparse_categorical_accuracy: 0.3281 - val_loss: 1.7328 - val_sparse_categorical_accuracy: 0.3079\n",
      "Epoch 302/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7071 - sparse_categorical_accuracy: 0.3264 - val_loss: 1.7499 - val_sparse_categorical_accuracy: 0.3039\n",
      "Epoch 303/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6978 - sparse_categorical_accuracy: 0.3195 - val_loss: 1.7409 - val_sparse_categorical_accuracy: 0.3183\n",
      "Epoch 304/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6913 - sparse_categorical_accuracy: 0.3472 - val_loss: 1.7248 - val_sparse_categorical_accuracy: 0.3360\n",
      "Epoch 305/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6867 - sparse_categorical_accuracy: 0.3372 - val_loss: 1.7491 - val_sparse_categorical_accuracy: 0.3232\n",
      "Epoch 306/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6738 - sparse_categorical_accuracy: 0.3398 - val_loss: 1.8095 - val_sparse_categorical_accuracy: 0.2789\n",
      "Epoch 307/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7027 - sparse_categorical_accuracy: 0.3299 - val_loss: 1.7306 - val_sparse_categorical_accuracy: 0.3039\n",
      "Epoch 308/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6862 - sparse_categorical_accuracy: 0.3294 - val_loss: 1.7397 - val_sparse_categorical_accuracy: 0.3127\n",
      "Epoch 309/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.6937 - sparse_categorical_accuracy: 0.3325 - val_loss: 1.7290 - val_sparse_categorical_accuracy: 0.3047\n",
      "Epoch 310/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6978 - sparse_categorical_accuracy: 0.3307 - val_loss: 1.7721 - val_sparse_categorical_accuracy: 0.2741\n",
      "Epoch 311/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7004 - sparse_categorical_accuracy: 0.3307 - val_loss: 1.7589 - val_sparse_categorical_accuracy: 0.3135\n",
      "Epoch 312/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6820 - sparse_categorical_accuracy: 0.3450 - val_loss: 1.7661 - val_sparse_categorical_accuracy: 0.3175\n",
      "Epoch 313/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7027 - sparse_categorical_accuracy: 0.3229 - val_loss: 1.7241 - val_sparse_categorical_accuracy: 0.3248\n",
      "Epoch 314/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6967 - sparse_categorical_accuracy: 0.3342 - val_loss: 1.7827 - val_sparse_categorical_accuracy: 0.3183\n",
      "Epoch 315/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6839 - sparse_categorical_accuracy: 0.3294 - val_loss: 1.7538 - val_sparse_categorical_accuracy: 0.3039\n",
      "Epoch 316/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6940 - sparse_categorical_accuracy: 0.3294 - val_loss: 1.7380 - val_sparse_categorical_accuracy: 0.3175\n",
      "Epoch 317/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6915 - sparse_categorical_accuracy: 0.3320 - val_loss: 1.7815 - val_sparse_categorical_accuracy: 0.2773\n",
      "Epoch 318/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7089 - sparse_categorical_accuracy: 0.3268 - val_loss: 1.7452 - val_sparse_categorical_accuracy: 0.3103\n",
      "Epoch 319/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6929 - sparse_categorical_accuracy: 0.3203 - val_loss: 1.7194 - val_sparse_categorical_accuracy: 0.3368\n",
      "Epoch 320/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6907 - sparse_categorical_accuracy: 0.3372 - val_loss: 1.7301 - val_sparse_categorical_accuracy: 0.2982\n",
      "Epoch 321/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7013 - sparse_categorical_accuracy: 0.3208 - val_loss: 1.7363 - val_sparse_categorical_accuracy: 0.3199\n",
      "Epoch 322/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7009 - sparse_categorical_accuracy: 0.3251 - val_loss: 1.7688 - val_sparse_categorical_accuracy: 0.3215\n",
      "Epoch 323/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6781 - sparse_categorical_accuracy: 0.3377 - val_loss: 1.7540 - val_sparse_categorical_accuracy: 0.3119\n",
      "Epoch 324/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6942 - sparse_categorical_accuracy: 0.3299 - val_loss: 1.7428 - val_sparse_categorical_accuracy: 0.3023\n",
      "Epoch 325/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6883 - sparse_categorical_accuracy: 0.3511 - val_loss: 1.7211 - val_sparse_categorical_accuracy: 0.3199\n",
      "Epoch 326/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6975 - sparse_categorical_accuracy: 0.3338 - val_loss: 1.7781 - val_sparse_categorical_accuracy: 0.2990\n",
      "Epoch 327/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6808 - sparse_categorical_accuracy: 0.3372 - val_loss: 1.7653 - val_sparse_categorical_accuracy: 0.2894\n",
      "Epoch 328/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6957 - sparse_categorical_accuracy: 0.3273 - val_loss: 1.7724 - val_sparse_categorical_accuracy: 0.2926\n",
      "Epoch 329/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6888 - sparse_categorical_accuracy: 0.3329 - val_loss: 1.7461 - val_sparse_categorical_accuracy: 0.3127\n",
      "Epoch 330/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6924 - sparse_categorical_accuracy: 0.3307 - val_loss: 1.7638 - val_sparse_categorical_accuracy: 0.3039\n",
      "Epoch 331/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6899 - sparse_categorical_accuracy: 0.3429 - val_loss: 1.7193 - val_sparse_categorical_accuracy: 0.3280\n",
      "Epoch 332/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7122 - sparse_categorical_accuracy: 0.3286 - val_loss: 1.7710 - val_sparse_categorical_accuracy: 0.2958\n",
      "Epoch 333/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6863 - sparse_categorical_accuracy: 0.3316 - val_loss: 1.7171 - val_sparse_categorical_accuracy: 0.3272\n",
      "Epoch 334/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6941 - sparse_categorical_accuracy: 0.3203 - val_loss: 1.7595 - val_sparse_categorical_accuracy: 0.2870\n",
      "Epoch 335/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6854 - sparse_categorical_accuracy: 0.3299 - val_loss: 1.7221 - val_sparse_categorical_accuracy: 0.3151\n",
      "Epoch 336/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6985 - sparse_categorical_accuracy: 0.3416 - val_loss: 1.7477 - val_sparse_categorical_accuracy: 0.3031\n",
      "Epoch 337/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6871 - sparse_categorical_accuracy: 0.3377 - val_loss: 1.7307 - val_sparse_categorical_accuracy: 0.3167\n",
      "Epoch 338/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7030 - sparse_categorical_accuracy: 0.3342 - val_loss: 1.7174 - val_sparse_categorical_accuracy: 0.3215\n",
      "Epoch 339/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6886 - sparse_categorical_accuracy: 0.3247 - val_loss: 1.7437 - val_sparse_categorical_accuracy: 0.2958\n",
      "Epoch 340/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6871 - sparse_categorical_accuracy: 0.3437 - val_loss: 1.7140 - val_sparse_categorical_accuracy: 0.3304\n",
      "Epoch 341/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6878 - sparse_categorical_accuracy: 0.3216 - val_loss: 1.7147 - val_sparse_categorical_accuracy: 0.3215\n",
      "Epoch 342/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6826 - sparse_categorical_accuracy: 0.3424 - val_loss: 1.7065 - val_sparse_categorical_accuracy: 0.3312\n",
      "Epoch 343/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7004 - sparse_categorical_accuracy: 0.3212 - val_loss: 1.7107 - val_sparse_categorical_accuracy: 0.3344\n",
      "Epoch 344/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6833 - sparse_categorical_accuracy: 0.3390 - val_loss: 1.7367 - val_sparse_categorical_accuracy: 0.3183\n",
      "Epoch 345/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6802 - sparse_categorical_accuracy: 0.3286 - val_loss: 1.7264 - val_sparse_categorical_accuracy: 0.3232\n",
      "Epoch 346/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6870 - sparse_categorical_accuracy: 0.3307 - val_loss: 1.7375 - val_sparse_categorical_accuracy: 0.3280\n",
      "Epoch 347/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6877 - sparse_categorical_accuracy: 0.3381 - val_loss: 1.7245 - val_sparse_categorical_accuracy: 0.3175\n",
      "Epoch 348/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6892 - sparse_categorical_accuracy: 0.3459 - val_loss: 1.7234 - val_sparse_categorical_accuracy: 0.3191\n",
      "Epoch 349/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6998 - sparse_categorical_accuracy: 0.3294 - val_loss: 1.7227 - val_sparse_categorical_accuracy: 0.3272\n",
      "Epoch 350/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6830 - sparse_categorical_accuracy: 0.3390 - val_loss: 1.7153 - val_sparse_categorical_accuracy: 0.3280\n",
      "Epoch 351/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6801 - sparse_categorical_accuracy: 0.3390 - val_loss: 1.7655 - val_sparse_categorical_accuracy: 0.2982\n",
      "Epoch 352/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6971 - sparse_categorical_accuracy: 0.3290 - val_loss: 1.7435 - val_sparse_categorical_accuracy: 0.3223\n",
      "Epoch 353/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6935 - sparse_categorical_accuracy: 0.3177 - val_loss: 1.7366 - val_sparse_categorical_accuracy: 0.3119\n",
      "Epoch 354/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6836 - sparse_categorical_accuracy: 0.3377 - val_loss: 1.7572 - val_sparse_categorical_accuracy: 0.3240\n",
      "Epoch 355/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6800 - sparse_categorical_accuracy: 0.3359 - val_loss: 1.7421 - val_sparse_categorical_accuracy: 0.3127\n",
      "Epoch 356/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6887 - sparse_categorical_accuracy: 0.3165 - val_loss: 1.7409 - val_sparse_categorical_accuracy: 0.3103\n",
      "Epoch 357/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6992 - sparse_categorical_accuracy: 0.3273 - val_loss: 1.7275 - val_sparse_categorical_accuracy: 0.3320\n",
      "Epoch 358/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6828 - sparse_categorical_accuracy: 0.3416 - val_loss: 1.7500 - val_sparse_categorical_accuracy: 0.3079\n",
      "Epoch 359/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6945 - sparse_categorical_accuracy: 0.3229 - val_loss: 1.7564 - val_sparse_categorical_accuracy: 0.2958\n",
      "Epoch 360/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6577 - sparse_categorical_accuracy: 0.3429 - val_loss: 1.7320 - val_sparse_categorical_accuracy: 0.2990\n",
      "Epoch 361/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6655 - sparse_categorical_accuracy: 0.3515 - val_loss: 1.7721 - val_sparse_categorical_accuracy: 0.2838\n",
      "Epoch 362/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6839 - sparse_categorical_accuracy: 0.3338 - val_loss: 1.7406 - val_sparse_categorical_accuracy: 0.3280\n",
      "Epoch 363/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6758 - sparse_categorical_accuracy: 0.3511 - val_loss: 1.7075 - val_sparse_categorical_accuracy: 0.3304\n",
      "Epoch 364/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6892 - sparse_categorical_accuracy: 0.3346 - val_loss: 1.7130 - val_sparse_categorical_accuracy: 0.3087\n",
      "Epoch 365/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6895 - sparse_categorical_accuracy: 0.3329 - val_loss: 1.7344 - val_sparse_categorical_accuracy: 0.3039\n",
      "Epoch 366/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.6991 - sparse_categorical_accuracy: 0.3398 - val_loss: 1.7251 - val_sparse_categorical_accuracy: 0.3031\n",
      "Epoch 367/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6745 - sparse_categorical_accuracy: 0.3333 - val_loss: 1.7518 - val_sparse_categorical_accuracy: 0.3223\n",
      "Epoch 368/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6827 - sparse_categorical_accuracy: 0.3450 - val_loss: 1.7195 - val_sparse_categorical_accuracy: 0.3240\n",
      "Epoch 369/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6809 - sparse_categorical_accuracy: 0.3403 - val_loss: 1.7820 - val_sparse_categorical_accuracy: 0.2709\n",
      "Epoch 370/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6763 - sparse_categorical_accuracy: 0.3420 - val_loss: 1.7486 - val_sparse_categorical_accuracy: 0.3328\n",
      "Epoch 371/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6737 - sparse_categorical_accuracy: 0.3351 - val_loss: 1.7397 - val_sparse_categorical_accuracy: 0.2974\n",
      "Epoch 372/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6827 - sparse_categorical_accuracy: 0.3234 - val_loss: 1.7169 - val_sparse_categorical_accuracy: 0.3272\n",
      "Epoch 373/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.7007 - sparse_categorical_accuracy: 0.3329 - val_loss: 1.7141 - val_sparse_categorical_accuracy: 0.3408\n",
      "Epoch 374/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6893 - sparse_categorical_accuracy: 0.3364 - val_loss: 1.7261 - val_sparse_categorical_accuracy: 0.3031\n",
      "Epoch 375/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6773 - sparse_categorical_accuracy: 0.3351 - val_loss: 1.7395 - val_sparse_categorical_accuracy: 0.3175\n",
      "Epoch 376/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6921 - sparse_categorical_accuracy: 0.3377 - val_loss: 1.7882 - val_sparse_categorical_accuracy: 0.2814\n",
      "Epoch 377/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6927 - sparse_categorical_accuracy: 0.3312 - val_loss: 1.7361 - val_sparse_categorical_accuracy: 0.3127\n",
      "Epoch 378/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6906 - sparse_categorical_accuracy: 0.3299 - val_loss: 1.7188 - val_sparse_categorical_accuracy: 0.3006\n",
      "Epoch 379/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6892 - sparse_categorical_accuracy: 0.3377 - val_loss: 1.7219 - val_sparse_categorical_accuracy: 0.3151\n",
      "Epoch 380/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6883 - sparse_categorical_accuracy: 0.3459 - val_loss: 1.7348 - val_sparse_categorical_accuracy: 0.3111\n",
      "Epoch 381/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6926 - sparse_categorical_accuracy: 0.3351 - val_loss: 1.7012 - val_sparse_categorical_accuracy: 0.3304\n",
      "Epoch 382/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6901 - sparse_categorical_accuracy: 0.3281 - val_loss: 1.7211 - val_sparse_categorical_accuracy: 0.3248\n",
      "Epoch 383/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6900 - sparse_categorical_accuracy: 0.3407 - val_loss: 1.7129 - val_sparse_categorical_accuracy: 0.3352\n",
      "Epoch 384/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6680 - sparse_categorical_accuracy: 0.3524 - val_loss: 1.7551 - val_sparse_categorical_accuracy: 0.2966\n",
      "Epoch 385/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6801 - sparse_categorical_accuracy: 0.3545 - val_loss: 1.7403 - val_sparse_categorical_accuracy: 0.3031\n",
      "Epoch 386/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6638 - sparse_categorical_accuracy: 0.3593 - val_loss: 1.7117 - val_sparse_categorical_accuracy: 0.3159\n",
      "Epoch 387/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6610 - sparse_categorical_accuracy: 0.3532 - val_loss: 1.7270 - val_sparse_categorical_accuracy: 0.3256\n",
      "Epoch 388/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6916 - sparse_categorical_accuracy: 0.3312 - val_loss: 1.7858 - val_sparse_categorical_accuracy: 0.2789\n",
      "Epoch 389/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6732 - sparse_categorical_accuracy: 0.3407 - val_loss: 1.7092 - val_sparse_categorical_accuracy: 0.3232\n",
      "Epoch 390/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6866 - sparse_categorical_accuracy: 0.3446 - val_loss: 1.7420 - val_sparse_categorical_accuracy: 0.3127\n",
      "Epoch 391/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6923 - sparse_categorical_accuracy: 0.3268 - val_loss: 1.7064 - val_sparse_categorical_accuracy: 0.3376\n",
      "Epoch 392/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6806 - sparse_categorical_accuracy: 0.3381 - val_loss: 1.7483 - val_sparse_categorical_accuracy: 0.3223\n",
      "Epoch 393/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6789 - sparse_categorical_accuracy: 0.3463 - val_loss: 1.7089 - val_sparse_categorical_accuracy: 0.3240\n",
      "Epoch 394/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6668 - sparse_categorical_accuracy: 0.3346 - val_loss: 1.7533 - val_sparse_categorical_accuracy: 0.3223\n",
      "Epoch 395/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6736 - sparse_categorical_accuracy: 0.3446 - val_loss: 1.7405 - val_sparse_categorical_accuracy: 0.3215\n",
      "Epoch 396/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6849 - sparse_categorical_accuracy: 0.3338 - val_loss: 1.7314 - val_sparse_categorical_accuracy: 0.2910\n",
      "Epoch 397/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6928 - sparse_categorical_accuracy: 0.3364 - val_loss: 1.7004 - val_sparse_categorical_accuracy: 0.3392\n",
      "Epoch 398/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6808 - sparse_categorical_accuracy: 0.3416 - val_loss: 1.7039 - val_sparse_categorical_accuracy: 0.3441\n",
      "Epoch 399/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6680 - sparse_categorical_accuracy: 0.3459 - val_loss: 1.7469 - val_sparse_categorical_accuracy: 0.3135\n",
      "Epoch 400/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6759 - sparse_categorical_accuracy: 0.3584 - val_loss: 1.7085 - val_sparse_categorical_accuracy: 0.3159\n",
      "Epoch 401/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6855 - sparse_categorical_accuracy: 0.3442 - val_loss: 1.6934 - val_sparse_categorical_accuracy: 0.3256\n",
      "Epoch 402/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6842 - sparse_categorical_accuracy: 0.3372 - val_loss: 1.7145 - val_sparse_categorical_accuracy: 0.3232\n",
      "Epoch 403/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6839 - sparse_categorical_accuracy: 0.3368 - val_loss: 1.7244 - val_sparse_categorical_accuracy: 0.3529\n",
      "Epoch 404/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6863 - sparse_categorical_accuracy: 0.3303 - val_loss: 1.7249 - val_sparse_categorical_accuracy: 0.3248\n",
      "Epoch 405/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6796 - sparse_categorical_accuracy: 0.3394 - val_loss: 1.7935 - val_sparse_categorical_accuracy: 0.2685\n",
      "Epoch 406/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6758 - sparse_categorical_accuracy: 0.3429 - val_loss: 1.7108 - val_sparse_categorical_accuracy: 0.3111\n",
      "Epoch 407/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6760 - sparse_categorical_accuracy: 0.3459 - val_loss: 1.7100 - val_sparse_categorical_accuracy: 0.3320\n",
      "Epoch 408/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6926 - sparse_categorical_accuracy: 0.3333 - val_loss: 1.7371 - val_sparse_categorical_accuracy: 0.3023\n",
      "Epoch 409/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6843 - sparse_categorical_accuracy: 0.3407 - val_loss: 1.7151 - val_sparse_categorical_accuracy: 0.3199\n",
      "Epoch 410/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6722 - sparse_categorical_accuracy: 0.3563 - val_loss: 1.7052 - val_sparse_categorical_accuracy: 0.3175\n",
      "Epoch 411/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6669 - sparse_categorical_accuracy: 0.3485 - val_loss: 1.7408 - val_sparse_categorical_accuracy: 0.3151\n",
      "Epoch 412/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6917 - sparse_categorical_accuracy: 0.3351 - val_loss: 1.7216 - val_sparse_categorical_accuracy: 0.3087\n",
      "Epoch 413/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6603 - sparse_categorical_accuracy: 0.3567 - val_loss: 1.7049 - val_sparse_categorical_accuracy: 0.3432\n",
      "Epoch 414/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6726 - sparse_categorical_accuracy: 0.3420 - val_loss: 1.7015 - val_sparse_categorical_accuracy: 0.3441\n",
      "Epoch 415/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6625 - sparse_categorical_accuracy: 0.3442 - val_loss: 1.7126 - val_sparse_categorical_accuracy: 0.3167\n",
      "Epoch 416/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6770 - sparse_categorical_accuracy: 0.3368 - val_loss: 1.7201 - val_sparse_categorical_accuracy: 0.3199\n",
      "Epoch 417/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6891 - sparse_categorical_accuracy: 0.3320 - val_loss: 1.7017 - val_sparse_categorical_accuracy: 0.3296\n",
      "Epoch 418/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6767 - sparse_categorical_accuracy: 0.3429 - val_loss: 1.7244 - val_sparse_categorical_accuracy: 0.2990\n",
      "Epoch 419/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6755 - sparse_categorical_accuracy: 0.3346 - val_loss: 1.7227 - val_sparse_categorical_accuracy: 0.3183\n",
      "Epoch 420/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6661 - sparse_categorical_accuracy: 0.3615 - val_loss: 1.7402 - val_sparse_categorical_accuracy: 0.2998\n",
      "Epoch 421/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6885 - sparse_categorical_accuracy: 0.3355 - val_loss: 1.7564 - val_sparse_categorical_accuracy: 0.3047\n",
      "Epoch 422/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6539 - sparse_categorical_accuracy: 0.3468 - val_loss: 1.7310 - val_sparse_categorical_accuracy: 0.3223\n",
      "Epoch 423/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6637 - sparse_categorical_accuracy: 0.3416 - val_loss: 1.7107 - val_sparse_categorical_accuracy: 0.3288\n",
      "Epoch 424/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6641 - sparse_categorical_accuracy: 0.3485 - val_loss: 1.7117 - val_sparse_categorical_accuracy: 0.3199\n",
      "Epoch 425/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6750 - sparse_categorical_accuracy: 0.3437 - val_loss: 1.6803 - val_sparse_categorical_accuracy: 0.3264\n",
      "Epoch 426/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6524 - sparse_categorical_accuracy: 0.3619 - val_loss: 1.7175 - val_sparse_categorical_accuracy: 0.3312\n",
      "Epoch 427/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6823 - sparse_categorical_accuracy: 0.3398 - val_loss: 1.7338 - val_sparse_categorical_accuracy: 0.3006\n",
      "Epoch 428/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6672 - sparse_categorical_accuracy: 0.3420 - val_loss: 1.7795 - val_sparse_categorical_accuracy: 0.2926\n",
      "Epoch 429/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6611 - sparse_categorical_accuracy: 0.3524 - val_loss: 1.7006 - val_sparse_categorical_accuracy: 0.3151\n",
      "Epoch 430/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6879 - sparse_categorical_accuracy: 0.3281 - val_loss: 1.7095 - val_sparse_categorical_accuracy: 0.3328\n",
      "Epoch 431/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6591 - sparse_categorical_accuracy: 0.3541 - val_loss: 1.7338 - val_sparse_categorical_accuracy: 0.3135\n",
      "Epoch 432/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6793 - sparse_categorical_accuracy: 0.3346 - val_loss: 1.6949 - val_sparse_categorical_accuracy: 0.3288\n",
      "Epoch 433/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6813 - sparse_categorical_accuracy: 0.3316 - val_loss: 1.7405 - val_sparse_categorical_accuracy: 0.3047\n",
      "Epoch 434/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6686 - sparse_categorical_accuracy: 0.3394 - val_loss: 1.6908 - val_sparse_categorical_accuracy: 0.3376\n",
      "Epoch 435/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6713 - sparse_categorical_accuracy: 0.3368 - val_loss: 1.7314 - val_sparse_categorical_accuracy: 0.3191\n",
      "Epoch 436/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6667 - sparse_categorical_accuracy: 0.3429 - val_loss: 1.6962 - val_sparse_categorical_accuracy: 0.3264\n",
      "Epoch 437/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.6786 - sparse_categorical_accuracy: 0.3446 - val_loss: 1.6987 - val_sparse_categorical_accuracy: 0.3521\n",
      "Epoch 438/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6852 - sparse_categorical_accuracy: 0.3429 - val_loss: 1.7151 - val_sparse_categorical_accuracy: 0.3360\n",
      "Epoch 439/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6665 - sparse_categorical_accuracy: 0.3411 - val_loss: 1.7231 - val_sparse_categorical_accuracy: 0.2830\n",
      "Epoch 440/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6701 - sparse_categorical_accuracy: 0.3359 - val_loss: 1.7241 - val_sparse_categorical_accuracy: 0.3143\n",
      "Epoch 441/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6832 - sparse_categorical_accuracy: 0.3446 - val_loss: 1.7967 - val_sparse_categorical_accuracy: 0.3014\n",
      "Epoch 442/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6643 - sparse_categorical_accuracy: 0.3563 - val_loss: 1.7109 - val_sparse_categorical_accuracy: 0.3223\n",
      "Epoch 443/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6695 - sparse_categorical_accuracy: 0.3416 - val_loss: 1.6856 - val_sparse_categorical_accuracy: 0.3424\n",
      "Epoch 444/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6844 - sparse_categorical_accuracy: 0.3424 - val_loss: 1.7141 - val_sparse_categorical_accuracy: 0.3095\n",
      "Epoch 445/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6802 - sparse_categorical_accuracy: 0.3346 - val_loss: 1.7390 - val_sparse_categorical_accuracy: 0.3023\n",
      "Epoch 446/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6712 - sparse_categorical_accuracy: 0.3433 - val_loss: 1.7125 - val_sparse_categorical_accuracy: 0.3183\n",
      "Epoch 447/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6845 - sparse_categorical_accuracy: 0.3342 - val_loss: 1.7229 - val_sparse_categorical_accuracy: 0.2998\n",
      "Epoch 448/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6752 - sparse_categorical_accuracy: 0.3403 - val_loss: 1.7229 - val_sparse_categorical_accuracy: 0.3240\n",
      "Epoch 449/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6720 - sparse_categorical_accuracy: 0.3390 - val_loss: 1.6998 - val_sparse_categorical_accuracy: 0.3248\n",
      "Epoch 450/600\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 1.6630 - sparse_categorical_accuracy: 0.3433 - val_loss: 1.7108 - val_sparse_categorical_accuracy: 0.3079\n",
      "Epoch 451/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6830 - sparse_categorical_accuracy: 0.3394 - val_loss: 1.6945 - val_sparse_categorical_accuracy: 0.3248\n",
      "Epoch 452/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6991 - sparse_categorical_accuracy: 0.3394 - val_loss: 1.7296 - val_sparse_categorical_accuracy: 0.3014\n",
      "Epoch 453/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6782 - sparse_categorical_accuracy: 0.3407 - val_loss: 1.7107 - val_sparse_categorical_accuracy: 0.3264\n",
      "Epoch 454/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6733 - sparse_categorical_accuracy: 0.3481 - val_loss: 1.6875 - val_sparse_categorical_accuracy: 0.3424\n",
      "Epoch 455/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6612 - sparse_categorical_accuracy: 0.3511 - val_loss: 1.7123 - val_sparse_categorical_accuracy: 0.3336\n",
      "Epoch 456/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6689 - sparse_categorical_accuracy: 0.3524 - val_loss: 1.7420 - val_sparse_categorical_accuracy: 0.3095\n",
      "Epoch 457/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6559 - sparse_categorical_accuracy: 0.3472 - val_loss: 1.6912 - val_sparse_categorical_accuracy: 0.3288\n",
      "Epoch 458/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6821 - sparse_categorical_accuracy: 0.3442 - val_loss: 1.7095 - val_sparse_categorical_accuracy: 0.3400\n",
      "Epoch 459/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6773 - sparse_categorical_accuracy: 0.3377 - val_loss: 1.7242 - val_sparse_categorical_accuracy: 0.3432\n",
      "Epoch 460/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6662 - sparse_categorical_accuracy: 0.3446 - val_loss: 1.7204 - val_sparse_categorical_accuracy: 0.3191\n",
      "Epoch 461/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6588 - sparse_categorical_accuracy: 0.3446 - val_loss: 1.7080 - val_sparse_categorical_accuracy: 0.3320\n",
      "Epoch 462/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6667 - sparse_categorical_accuracy: 0.3377 - val_loss: 1.7259 - val_sparse_categorical_accuracy: 0.3135\n",
      "Epoch 463/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6895 - sparse_categorical_accuracy: 0.3355 - val_loss: 1.7451 - val_sparse_categorical_accuracy: 0.3014\n",
      "Epoch 464/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6748 - sparse_categorical_accuracy: 0.3463 - val_loss: 1.7216 - val_sparse_categorical_accuracy: 0.3159\n",
      "Epoch 465/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6619 - sparse_categorical_accuracy: 0.3502 - val_loss: 1.6804 - val_sparse_categorical_accuracy: 0.3191\n",
      "Epoch 466/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6686 - sparse_categorical_accuracy: 0.3429 - val_loss: 1.7275 - val_sparse_categorical_accuracy: 0.3103\n",
      "Epoch 467/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6659 - sparse_categorical_accuracy: 0.3398 - val_loss: 1.7129 - val_sparse_categorical_accuracy: 0.3336\n",
      "Epoch 468/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6622 - sparse_categorical_accuracy: 0.3463 - val_loss: 1.6930 - val_sparse_categorical_accuracy: 0.3368\n",
      "Epoch 469/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6819 - sparse_categorical_accuracy: 0.3407 - val_loss: 1.7078 - val_sparse_categorical_accuracy: 0.3344\n",
      "Epoch 470/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6623 - sparse_categorical_accuracy: 0.3511 - val_loss: 1.7215 - val_sparse_categorical_accuracy: 0.3296\n",
      "Epoch 471/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6748 - sparse_categorical_accuracy: 0.3286 - val_loss: 1.7116 - val_sparse_categorical_accuracy: 0.3368\n",
      "Epoch 472/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6499 - sparse_categorical_accuracy: 0.3541 - val_loss: 1.6938 - val_sparse_categorical_accuracy: 0.3191\n",
      "Epoch 473/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6845 - sparse_categorical_accuracy: 0.3390 - val_loss: 1.6949 - val_sparse_categorical_accuracy: 0.3151\n",
      "Epoch 474/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6630 - sparse_categorical_accuracy: 0.3459 - val_loss: 1.7481 - val_sparse_categorical_accuracy: 0.2990\n",
      "Epoch 475/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6887 - sparse_categorical_accuracy: 0.3394 - val_loss: 1.7194 - val_sparse_categorical_accuracy: 0.3031\n",
      "Epoch 476/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6743 - sparse_categorical_accuracy: 0.3329 - val_loss: 1.7025 - val_sparse_categorical_accuracy: 0.3352\n",
      "Epoch 477/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6776 - sparse_categorical_accuracy: 0.3472 - val_loss: 1.7079 - val_sparse_categorical_accuracy: 0.3288\n",
      "Epoch 478/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6685 - sparse_categorical_accuracy: 0.3381 - val_loss: 1.6934 - val_sparse_categorical_accuracy: 0.3384\n",
      "Epoch 479/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6666 - sparse_categorical_accuracy: 0.3528 - val_loss: 1.7104 - val_sparse_categorical_accuracy: 0.3344\n",
      "Epoch 480/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6733 - sparse_categorical_accuracy: 0.3429 - val_loss: 1.7216 - val_sparse_categorical_accuracy: 0.3167\n",
      "Epoch 481/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6671 - sparse_categorical_accuracy: 0.3532 - val_loss: 1.7060 - val_sparse_categorical_accuracy: 0.3215\n",
      "Epoch 482/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6814 - sparse_categorical_accuracy: 0.3450 - val_loss: 1.6946 - val_sparse_categorical_accuracy: 0.3304\n",
      "Epoch 483/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6656 - sparse_categorical_accuracy: 0.3515 - val_loss: 1.7096 - val_sparse_categorical_accuracy: 0.3215\n",
      "Epoch 484/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6613 - sparse_categorical_accuracy: 0.3359 - val_loss: 1.6899 - val_sparse_categorical_accuracy: 0.3223\n",
      "Epoch 485/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6703 - sparse_categorical_accuracy: 0.3403 - val_loss: 1.6984 - val_sparse_categorical_accuracy: 0.3296\n",
      "Epoch 486/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6643 - sparse_categorical_accuracy: 0.3390 - val_loss: 1.7677 - val_sparse_categorical_accuracy: 0.3087\n",
      "Epoch 487/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6707 - sparse_categorical_accuracy: 0.3429 - val_loss: 1.6944 - val_sparse_categorical_accuracy: 0.3360\n",
      "Epoch 488/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6789 - sparse_categorical_accuracy: 0.3528 - val_loss: 1.7021 - val_sparse_categorical_accuracy: 0.3264\n",
      "Epoch 489/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6539 - sparse_categorical_accuracy: 0.3403 - val_loss: 1.7202 - val_sparse_categorical_accuracy: 0.3063\n",
      "Epoch 490/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6752 - sparse_categorical_accuracy: 0.3385 - val_loss: 1.7005 - val_sparse_categorical_accuracy: 0.3127\n",
      "Epoch 491/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6733 - sparse_categorical_accuracy: 0.3411 - val_loss: 1.7390 - val_sparse_categorical_accuracy: 0.3352\n",
      "Epoch 492/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6497 - sparse_categorical_accuracy: 0.3550 - val_loss: 1.7029 - val_sparse_categorical_accuracy: 0.3473\n",
      "Epoch 493/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6700 - sparse_categorical_accuracy: 0.3446 - val_loss: 1.7384 - val_sparse_categorical_accuracy: 0.3167\n",
      "Epoch 494/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6875 - sparse_categorical_accuracy: 0.3442 - val_loss: 1.7023 - val_sparse_categorical_accuracy: 0.3368\n",
      "Epoch 495/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6637 - sparse_categorical_accuracy: 0.3554 - val_loss: 1.7392 - val_sparse_categorical_accuracy: 0.2990\n",
      "Epoch 496/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6704 - sparse_categorical_accuracy: 0.3407 - val_loss: 1.7370 - val_sparse_categorical_accuracy: 0.3111\n",
      "Epoch 497/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6854 - sparse_categorical_accuracy: 0.3494 - val_loss: 1.7369 - val_sparse_categorical_accuracy: 0.3023\n",
      "Epoch 498/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6746 - sparse_categorical_accuracy: 0.3442 - val_loss: 1.7079 - val_sparse_categorical_accuracy: 0.3336\n",
      "Epoch 499/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6571 - sparse_categorical_accuracy: 0.3558 - val_loss: 1.7321 - val_sparse_categorical_accuracy: 0.3006\n",
      "Epoch 500/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6669 - sparse_categorical_accuracy: 0.3468 - val_loss: 1.7157 - val_sparse_categorical_accuracy: 0.3175\n",
      "Epoch 501/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6769 - sparse_categorical_accuracy: 0.3437 - val_loss: 1.7494 - val_sparse_categorical_accuracy: 0.3111\n",
      "Epoch 502/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6655 - sparse_categorical_accuracy: 0.3455 - val_loss: 1.7206 - val_sparse_categorical_accuracy: 0.3280\n",
      "Epoch 503/600\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6659 - sparse_categorical_accuracy: 0.3502 - val_loss: 1.6941 - val_sparse_categorical_accuracy: 0.3232\n",
      "Accuracy of the model on test set: 35.289%\n",
      "Accuracy of the model on validation set: 13.990%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "np.random.seed = 123\n",
    "\n",
    "y = Data_train.Emotion\n",
    "X = pd.DataFrame(Data_train.drop(['Emotion','User'], axis = 1))\n",
    "\n",
    "y_val = Data_val.Emotion\n",
    "X_val = pd.DataFrame(Data_val.drop(['Emotion', 'User'], axis = 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.35, random_state = 123)\n",
    "\n",
    "\n",
    "#Trying adam optimizer\n",
    "#model = initModelGRU(X.shape[1], 7, 'softmax') \n",
    "\n",
    "model = initModelBasic(X.shape[1], 7)\n",
    "model.compile(\n",
    "    optimizer= 'adam',\n",
    "    loss='sparse_categorical_crossentropy', # sparse because using integer labels for emotions (not OHE)\n",
    "    metrics=[ 'sparse_categorical_accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    #validation_split = 0.3,\n",
    "    #validation_data = (X_val, y_val),\n",
    "    validation_data = (X_test, y_test),\n",
    "    epochs=600,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_sparse_categorical_accuracy',\n",
    "            patience=100,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Result on test data\n",
    "model_acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print(\"Accuracy of the model on test set: {:.3f}%\".format(model_acc * 100))\n",
    "\n",
    "#Result on val data\n",
    "model_acc = model.evaluate(X_val, y_val, verbose=0)[1]\n",
    "print(\"Accuracy of the model on validation set: {:.3f}%\".format(model_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f77d41c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.03511801957397812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on test set: 18.408%\n",
      "Accuracy of the model on validation set: 18.538%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1737])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "#model.save('8_features_train_test_16.983')\n",
    "#test 60.691  val 16.983\n",
    "#model = load_model('8_features_train_test') #val 19%!\n",
    "\n",
    "acc =tf.keras.metrics.sparse_categorical_accuracy(y_val, model.predict(X_val))\n",
    "acc.numpy()\n",
    "print(\"acc: \", accuracy_score(y_val, acc))\n",
    "\n",
    "\n",
    "model_acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print(\"Accuracy of the model on test set: {:.3f}%\".format(model_acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "# Plot the accuracy curve for training\n",
    "#plt.plot(history.history['val_accuracy'], color='r', label=\"Accuracy\")\n",
    "\n",
    "plt.title(\"Validation accuracy\")\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "\n",
    "model_acc = model.evaluate(X_val, y_val, verbose=0)[1]\n",
    "print(\"Accuracy of the model on validation set: {:.3f}%\".format(model_acc * 100))\n",
    "\n",
    "acc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f2f7d63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 9)]               0         \n",
      "                                                                 \n",
      " tf.expand_dims (TFOpLambda)  (None, 9, 1)             0         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 9, 256)            198912    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 7)                 16135     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 215,047\n",
      "Trainable params: 215,047\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n",
      "38/38 [==============================] - 4s 47ms/step - loss: 1.9661 - accuracy: 0.1556 - val_loss: 1.9222 - val_accuracy: 0.1762\n",
      "Epoch 2/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.9319 - accuracy: 0.1835 - val_loss: 1.9217 - val_accuracy: 0.1820\n",
      "Epoch 3/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.9220 - accuracy: 0.1811 - val_loss: 1.9028 - val_accuracy: 0.2107\n",
      "Epoch 4/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.9161 - accuracy: 0.1942 - val_loss: 1.9073 - val_accuracy: 0.2165\n",
      "Epoch 5/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.9197 - accuracy: 0.1778 - val_loss: 1.9098 - val_accuracy: 0.2241\n",
      "Epoch 6/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.9153 - accuracy: 0.2074 - val_loss: 1.8978 - val_accuracy: 0.1935\n",
      "Epoch 7/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.9063 - accuracy: 0.1959 - val_loss: 1.8900 - val_accuracy: 0.2414\n",
      "Epoch 8/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.9046 - accuracy: 0.2008 - val_loss: 1.8857 - val_accuracy: 0.2280\n",
      "Epoch 9/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.8964 - accuracy: 0.2115 - val_loss: 1.8606 - val_accuracy: 0.2682\n",
      "Epoch 10/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.8970 - accuracy: 0.2049 - val_loss: 1.8765 - val_accuracy: 0.2222\n",
      "Epoch 11/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.8959 - accuracy: 0.2025 - val_loss: 1.8642 - val_accuracy: 0.2759\n",
      "Epoch 12/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.8876 - accuracy: 0.2354 - val_loss: 1.8639 - val_accuracy: 0.2395\n",
      "Epoch 13/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.8827 - accuracy: 0.2280 - val_loss: 1.8542 - val_accuracy: 0.2510\n",
      "Epoch 14/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.8719 - accuracy: 0.2255 - val_loss: 1.8634 - val_accuracy: 0.2490\n",
      "Epoch 15/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.8747 - accuracy: 0.2337 - val_loss: 1.8859 - val_accuracy: 0.2126\n",
      "Epoch 16/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.8710 - accuracy: 0.2494 - val_loss: 1.8212 - val_accuracy: 0.2682\n",
      "Epoch 17/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.8537 - accuracy: 0.2502 - val_loss: 1.8272 - val_accuracy: 0.2414\n",
      "Epoch 18/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.8570 - accuracy: 0.2519 - val_loss: 1.8242 - val_accuracy: 0.2625\n",
      "Epoch 19/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.8554 - accuracy: 0.2510 - val_loss: 1.8168 - val_accuracy: 0.3161\n",
      "Epoch 20/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.8521 - accuracy: 0.2494 - val_loss: 1.8458 - val_accuracy: 0.2816\n",
      "Epoch 21/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.8397 - accuracy: 0.2486 - val_loss: 1.8150 - val_accuracy: 0.2778\n",
      "Epoch 22/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.8284 - accuracy: 0.2724 - val_loss: 1.8165 - val_accuracy: 0.2931\n",
      "Epoch 23/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.8334 - accuracy: 0.2576 - val_loss: 1.8035 - val_accuracy: 0.2816\n",
      "Epoch 24/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.8292 - accuracy: 0.2626 - val_loss: 1.8061 - val_accuracy: 0.2720\n",
      "Epoch 25/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.8171 - accuracy: 0.2642 - val_loss: 1.7945 - val_accuracy: 0.3027\n",
      "Epoch 26/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.8026 - accuracy: 0.2840 - val_loss: 1.7890 - val_accuracy: 0.2701\n",
      "Epoch 27/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.7913 - accuracy: 0.2955 - val_loss: 1.7922 - val_accuracy: 0.2989\n",
      "Epoch 28/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 1.7942 - accuracy: 0.2856 - val_loss: 1.7721 - val_accuracy: 0.2778\n",
      "Epoch 29/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.7994 - accuracy: 0.2733 - val_loss: 1.8004 - val_accuracy: 0.3046\n",
      "Epoch 30/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.7787 - accuracy: 0.2922 - val_loss: 1.7713 - val_accuracy: 0.2950\n",
      "Epoch 31/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.7845 - accuracy: 0.2765 - val_loss: 1.7789 - val_accuracy: 0.3372\n",
      "Epoch 32/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.7715 - accuracy: 0.2914 - val_loss: 1.7928 - val_accuracy: 0.2739\n",
      "Epoch 33/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.7793 - accuracy: 0.2782 - val_loss: 1.7694 - val_accuracy: 0.2931\n",
      "Epoch 34/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.7575 - accuracy: 0.3111 - val_loss: 1.7505 - val_accuracy: 0.2874\n",
      "Epoch 35/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.7412 - accuracy: 0.2947 - val_loss: 1.7608 - val_accuracy: 0.3142\n",
      "Epoch 36/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.7309 - accuracy: 0.3062 - val_loss: 1.7236 - val_accuracy: 0.2912\n",
      "Epoch 37/1000\n",
      "38/38 [==============================] - 1s 40ms/step - loss: 1.7269 - accuracy: 0.3062 - val_loss: 1.7549 - val_accuracy: 0.2893\n",
      "Epoch 38/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 1.7238 - accuracy: 0.3045 - val_loss: 1.7565 - val_accuracy: 0.2893\n",
      "Epoch 39/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.7044 - accuracy: 0.3095 - val_loss: 1.7531 - val_accuracy: 0.2605\n",
      "Epoch 40/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 1.7036 - accuracy: 0.3226 - val_loss: 1.7157 - val_accuracy: 0.3276\n",
      "Epoch 41/1000\n",
      "38/38 [==============================] - 2s 41ms/step - loss: 1.6841 - accuracy: 0.3144 - val_loss: 1.6986 - val_accuracy: 0.3142\n",
      "Epoch 42/1000\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 1.6689 - accuracy: 0.3300 - val_loss: 1.7188 - val_accuracy: 0.3276\n",
      "Epoch 43/1000\n",
      "38/38 [==============================] - 2s 41ms/step - loss: 1.6689 - accuracy: 0.3185 - val_loss: 1.6751 - val_accuracy: 0.3525\n",
      "Epoch 44/1000\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 1.6648 - accuracy: 0.3267 - val_loss: 1.6855 - val_accuracy: 0.3257\n",
      "Epoch 45/1000\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 1.6372 - accuracy: 0.3366 - val_loss: 1.6817 - val_accuracy: 0.3276\n",
      "Epoch 46/1000\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 1.6425 - accuracy: 0.3267 - val_loss: 1.6552 - val_accuracy: 0.3640\n",
      "Epoch 47/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.6398 - accuracy: 0.3317 - val_loss: 1.6644 - val_accuracy: 0.3391\n",
      "Epoch 48/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.6132 - accuracy: 0.3481 - val_loss: 1.6434 - val_accuracy: 0.3716\n",
      "Epoch 49/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.5937 - accuracy: 0.3597 - val_loss: 1.6387 - val_accuracy: 0.3391\n",
      "Epoch 50/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.5942 - accuracy: 0.3457 - val_loss: 1.6051 - val_accuracy: 0.3525\n",
      "Epoch 51/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.5897 - accuracy: 0.3449 - val_loss: 1.6380 - val_accuracy: 0.3429\n",
      "Epoch 52/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.5639 - accuracy: 0.3728 - val_loss: 1.5930 - val_accuracy: 0.3544\n",
      "Epoch 53/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.5433 - accuracy: 0.3720 - val_loss: 1.6207 - val_accuracy: 0.3640\n",
      "Epoch 54/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.5675 - accuracy: 0.3663 - val_loss: 1.6187 - val_accuracy: 0.3602\n",
      "Epoch 55/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.5494 - accuracy: 0.3770 - val_loss: 1.5902 - val_accuracy: 0.3755\n",
      "Epoch 56/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.5262 - accuracy: 0.3712 - val_loss: 1.5532 - val_accuracy: 0.3640\n",
      "Epoch 57/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.5206 - accuracy: 0.3761 - val_loss: 1.5460 - val_accuracy: 0.4023\n",
      "Epoch 58/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.5175 - accuracy: 0.4016 - val_loss: 1.5668 - val_accuracy: 0.3602\n",
      "Epoch 59/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.5154 - accuracy: 0.3860 - val_loss: 1.5388 - val_accuracy: 0.3774\n",
      "Epoch 60/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.5000 - accuracy: 0.3893 - val_loss: 1.5381 - val_accuracy: 0.3774\n",
      "Epoch 61/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.4843 - accuracy: 0.4123 - val_loss: 1.5952 - val_accuracy: 0.3238\n",
      "Epoch 62/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.4773 - accuracy: 0.3959 - val_loss: 1.5598 - val_accuracy: 0.3448\n",
      "Epoch 63/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.4781 - accuracy: 0.4074 - val_loss: 1.4844 - val_accuracy: 0.4272\n",
      "Epoch 64/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.4428 - accuracy: 0.3992 - val_loss: 1.5117 - val_accuracy: 0.4368\n",
      "Epoch 65/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.4390 - accuracy: 0.3926 - val_loss: 1.5630 - val_accuracy: 0.3927\n",
      "Epoch 66/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.4525 - accuracy: 0.3951 - val_loss: 1.5329 - val_accuracy: 0.3621\n",
      "Epoch 67/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.4327 - accuracy: 0.4288 - val_loss: 1.5145 - val_accuracy: 0.3870\n",
      "Epoch 68/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.4532 - accuracy: 0.3934 - val_loss: 1.4805 - val_accuracy: 0.4330\n",
      "Epoch 69/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.4135 - accuracy: 0.4313 - val_loss: 1.4607 - val_accuracy: 0.4253\n",
      "Epoch 70/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.4071 - accuracy: 0.4214 - val_loss: 1.5036 - val_accuracy: 0.4042\n",
      "Epoch 71/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.4097 - accuracy: 0.4239 - val_loss: 1.4817 - val_accuracy: 0.4119\n",
      "Epoch 72/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.4084 - accuracy: 0.4091 - val_loss: 1.5304 - val_accuracy: 0.3870\n",
      "Epoch 73/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.3759 - accuracy: 0.4535 - val_loss: 1.4497 - val_accuracy: 0.4138\n",
      "Epoch 74/1000\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 1.3731 - accuracy: 0.4362 - val_loss: 1.4643 - val_accuracy: 0.4330\n",
      "Epoch 75/1000\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 1.3753 - accuracy: 0.4576 - val_loss: 1.4655 - val_accuracy: 0.4368\n",
      "Epoch 76/1000\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 1.3693 - accuracy: 0.4502 - val_loss: 1.4518 - val_accuracy: 0.4425\n",
      "Epoch 77/1000\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 1.3566 - accuracy: 0.4436 - val_loss: 1.4169 - val_accuracy: 0.4176\n",
      "Epoch 78/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.3651 - accuracy: 0.4387 - val_loss: 1.4515 - val_accuracy: 0.4004\n",
      "Epoch 79/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.3593 - accuracy: 0.4543 - val_loss: 1.4112 - val_accuracy: 0.4579\n",
      "Epoch 80/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.3385 - accuracy: 0.4543 - val_loss: 1.4286 - val_accuracy: 0.4502\n",
      "Epoch 81/1000\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 1.3449 - accuracy: 0.4568 - val_loss: 1.4086 - val_accuracy: 0.4272\n",
      "Epoch 82/1000\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 1.3381 - accuracy: 0.4601 - val_loss: 1.4298 - val_accuracy: 0.4540\n",
      "Epoch 83/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 1.3181 - accuracy: 0.4708 - val_loss: 1.3983 - val_accuracy: 0.4655\n",
      "Epoch 84/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.3264 - accuracy: 0.4658 - val_loss: 1.4243 - val_accuracy: 0.4004\n",
      "Epoch 85/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.3292 - accuracy: 0.4543 - val_loss: 1.4357 - val_accuracy: 0.4310\n",
      "Epoch 86/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 1.2780 - accuracy: 0.4872 - val_loss: 1.3853 - val_accuracy: 0.4406\n",
      "Epoch 87/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.2921 - accuracy: 0.4675 - val_loss: 1.4128 - val_accuracy: 0.4272\n",
      "Epoch 88/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.2789 - accuracy: 0.4765 - val_loss: 1.4092 - val_accuracy: 0.4444\n",
      "Epoch 89/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.2640 - accuracy: 0.4947 - val_loss: 1.3904 - val_accuracy: 0.4732\n",
      "Epoch 90/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.2964 - accuracy: 0.4642 - val_loss: 1.4073 - val_accuracy: 0.4655\n",
      "Epoch 91/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.2739 - accuracy: 0.4782 - val_loss: 1.3893 - val_accuracy: 0.4425\n",
      "Epoch 92/1000\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 1.2584 - accuracy: 0.4930 - val_loss: 1.4007 - val_accuracy: 0.4444\n",
      "Epoch 93/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.2714 - accuracy: 0.4848 - val_loss: 1.3558 - val_accuracy: 0.4847\n",
      "Epoch 94/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.2566 - accuracy: 0.4938 - val_loss: 1.3715 - val_accuracy: 0.4713\n",
      "Epoch 95/1000\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 1.2834 - accuracy: 0.4757 - val_loss: 1.3602 - val_accuracy: 0.4770\n",
      "Epoch 96/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.2656 - accuracy: 0.4864 - val_loss: 1.3667 - val_accuracy: 0.4713\n",
      "Epoch 97/1000\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 1.2624 - accuracy: 0.4905 - val_loss: 1.3592 - val_accuracy: 0.4483\n",
      "Epoch 98/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.2294 - accuracy: 0.4996 - val_loss: 1.3742 - val_accuracy: 0.4464\n",
      "Epoch 99/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.2351 - accuracy: 0.4971 - val_loss: 1.3108 - val_accuracy: 0.4962\n",
      "Epoch 100/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.2187 - accuracy: 0.5045 - val_loss: 1.2828 - val_accuracy: 0.5172\n",
      "Epoch 101/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 1.2010 - accuracy: 0.5021 - val_loss: 1.3359 - val_accuracy: 0.4483\n",
      "Epoch 102/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.2150 - accuracy: 0.5029 - val_loss: 1.3436 - val_accuracy: 0.4674\n",
      "Epoch 103/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.2199 - accuracy: 0.5226 - val_loss: 1.3288 - val_accuracy: 0.4981\n",
      "Epoch 104/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.2127 - accuracy: 0.5103 - val_loss: 1.3377 - val_accuracy: 0.4866\n",
      "Epoch 105/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.1954 - accuracy: 0.5119 - val_loss: 1.3275 - val_accuracy: 0.4713\n",
      "Epoch 106/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.1776 - accuracy: 0.5226 - val_loss: 1.3134 - val_accuracy: 0.5057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.2008 - accuracy: 0.5053 - val_loss: 1.3129 - val_accuracy: 0.4866\n",
      "Epoch 108/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.1968 - accuracy: 0.5251 - val_loss: 1.2973 - val_accuracy: 0.4789\n",
      "Epoch 109/1000\n",
      "38/38 [==============================] - 2s 57ms/step - loss: 1.1777 - accuracy: 0.5136 - val_loss: 1.3076 - val_accuracy: 0.5077\n",
      "Epoch 110/1000\n",
      "38/38 [==============================] - 3s 90ms/step - loss: 1.1519 - accuracy: 0.5284 - val_loss: 1.3027 - val_accuracy: 0.5077\n",
      "Epoch 111/1000\n",
      "38/38 [==============================] - 4s 103ms/step - loss: 1.1545 - accuracy: 0.5333 - val_loss: 1.3478 - val_accuracy: 0.5096\n",
      "Epoch 112/1000\n",
      "38/38 [==============================] - 3s 92ms/step - loss: 1.1566 - accuracy: 0.5202 - val_loss: 1.3114 - val_accuracy: 0.4981\n",
      "Epoch 113/1000\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 1.1667 - accuracy: 0.5284 - val_loss: 1.2720 - val_accuracy: 0.5268\n",
      "Epoch 114/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.1552 - accuracy: 0.5350 - val_loss: 1.2924 - val_accuracy: 0.5268\n",
      "Epoch 115/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 1.1688 - accuracy: 0.5202 - val_loss: 1.2600 - val_accuracy: 0.5230\n",
      "Epoch 116/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.1511 - accuracy: 0.5235 - val_loss: 1.2568 - val_accuracy: 0.5249\n",
      "Epoch 117/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.1509 - accuracy: 0.5465 - val_loss: 1.2540 - val_accuracy: 0.5172\n",
      "Epoch 118/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.1255 - accuracy: 0.5440 - val_loss: 1.3337 - val_accuracy: 0.5038\n",
      "Epoch 119/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 1.1638 - accuracy: 0.5243 - val_loss: 1.2599 - val_accuracy: 0.5192\n",
      "Epoch 120/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.1287 - accuracy: 0.5440 - val_loss: 1.3064 - val_accuracy: 0.4904\n",
      "Epoch 121/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 1.1218 - accuracy: 0.5374 - val_loss: 1.2939 - val_accuracy: 0.4904\n",
      "Epoch 122/1000\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 1.1298 - accuracy: 0.5391 - val_loss: 1.3516 - val_accuracy: 0.5000\n",
      "Epoch 123/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.1211 - accuracy: 0.5547 - val_loss: 1.2625 - val_accuracy: 0.5268\n",
      "Epoch 124/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.0960 - accuracy: 0.5613 - val_loss: 1.3190 - val_accuracy: 0.4904\n",
      "Epoch 125/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.1141 - accuracy: 0.5547 - val_loss: 1.2439 - val_accuracy: 0.5441\n",
      "Epoch 126/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.1070 - accuracy: 0.5440 - val_loss: 1.2560 - val_accuracy: 0.5192\n",
      "Epoch 127/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.1127 - accuracy: 0.5424 - val_loss: 1.3472 - val_accuracy: 0.4962\n",
      "Epoch 128/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.1242 - accuracy: 0.5597 - val_loss: 1.2408 - val_accuracy: 0.5460\n",
      "Epoch 129/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.0925 - accuracy: 0.5646 - val_loss: 1.3133 - val_accuracy: 0.5000\n",
      "Epoch 130/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.1287 - accuracy: 0.5399 - val_loss: 1.2655 - val_accuracy: 0.5077\n",
      "Epoch 131/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.1264 - accuracy: 0.5284 - val_loss: 1.2549 - val_accuracy: 0.5498\n",
      "Epoch 132/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.1118 - accuracy: 0.5490 - val_loss: 1.2450 - val_accuracy: 0.5498\n",
      "Epoch 133/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 1.0891 - accuracy: 0.5588 - val_loss: 1.3179 - val_accuracy: 0.5153\n",
      "Epoch 134/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.0706 - accuracy: 0.5539 - val_loss: 1.2552 - val_accuracy: 0.5402\n",
      "Epoch 135/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.1007 - accuracy: 0.5547 - val_loss: 1.2381 - val_accuracy: 0.5268\n",
      "Epoch 136/1000\n",
      "38/38 [==============================] - 2s 42ms/step - loss: 1.1005 - accuracy: 0.5473 - val_loss: 1.2956 - val_accuracy: 0.4828\n",
      "Epoch 137/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.0820 - accuracy: 0.5350 - val_loss: 1.2561 - val_accuracy: 0.5287\n",
      "Epoch 138/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.0846 - accuracy: 0.5317 - val_loss: 1.2588 - val_accuracy: 0.5517\n",
      "Epoch 139/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.0544 - accuracy: 0.5770 - val_loss: 1.2545 - val_accuracy: 0.5211\n",
      "Epoch 140/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.0725 - accuracy: 0.5737 - val_loss: 1.2638 - val_accuracy: 0.5134\n",
      "Epoch 141/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.0633 - accuracy: 0.5580 - val_loss: 1.2467 - val_accuracy: 0.5307\n",
      "Epoch 142/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.0681 - accuracy: 0.5588 - val_loss: 1.2845 - val_accuracy: 0.5211\n",
      "Epoch 143/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 1.0552 - accuracy: 0.5506 - val_loss: 1.2927 - val_accuracy: 0.5230\n",
      "Epoch 144/1000\n",
      "38/38 [==============================] - 2s 42ms/step - loss: 1.0753 - accuracy: 0.5490 - val_loss: 1.2338 - val_accuracy: 0.5479\n",
      "Epoch 145/1000\n",
      "38/38 [==============================] - 2s 43ms/step - loss: 1.0703 - accuracy: 0.5564 - val_loss: 1.2353 - val_accuracy: 0.5230\n",
      "Epoch 146/1000\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 1.0432 - accuracy: 0.5687 - val_loss: 1.2340 - val_accuracy: 0.5460\n",
      "Epoch 147/1000\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 1.0201 - accuracy: 0.5638 - val_loss: 1.2059 - val_accuracy: 0.5364\n",
      "Epoch 148/1000\n",
      "38/38 [==============================] - 5s 121ms/step - loss: 1.0312 - accuracy: 0.5794 - val_loss: 1.2329 - val_accuracy: 0.5326\n",
      "Epoch 149/1000\n",
      "38/38 [==============================] - 5s 129ms/step - loss: 1.0429 - accuracy: 0.5794 - val_loss: 1.2605 - val_accuracy: 0.5307\n",
      "Epoch 150/1000\n",
      "38/38 [==============================] - 5s 125ms/step - loss: 1.0575 - accuracy: 0.5605 - val_loss: 1.2892 - val_accuracy: 0.5230\n",
      "Epoch 151/1000\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 1.0228 - accuracy: 0.5868 - val_loss: 1.2095 - val_accuracy: 0.5517\n",
      "Epoch 152/1000\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 1.0542 - accuracy: 0.5712 - val_loss: 1.2583 - val_accuracy: 0.5211\n",
      "Epoch 153/1000\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 1.0466 - accuracy: 0.5621 - val_loss: 1.2808 - val_accuracy: 0.5268\n",
      "Epoch 154/1000\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 1.0291 - accuracy: 0.5753 - val_loss: 1.2825 - val_accuracy: 0.5192\n",
      "Epoch 155/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.0197 - accuracy: 0.5745 - val_loss: 1.2422 - val_accuracy: 0.5402\n",
      "Epoch 156/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.0166 - accuracy: 0.5663 - val_loss: 1.2389 - val_accuracy: 0.5670\n",
      "Epoch 157/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 1.0266 - accuracy: 0.5770 - val_loss: 1.2478 - val_accuracy: 0.5172\n",
      "Epoch 158/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.0267 - accuracy: 0.5613 - val_loss: 1.2382 - val_accuracy: 0.5460\n",
      "Epoch 159/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 1.0198 - accuracy: 0.5737 - val_loss: 1.2647 - val_accuracy: 0.5287\n",
      "Epoch 160/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.0273 - accuracy: 0.5802 - val_loss: 1.2397 - val_accuracy: 0.5345\n",
      "Epoch 161/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.0164 - accuracy: 0.5819 - val_loss: 1.2616 - val_accuracy: 0.5268\n",
      "Epoch 162/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.9815 - accuracy: 0.5951 - val_loss: 1.2715 - val_accuracy: 0.5479\n",
      "Epoch 163/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.9969 - accuracy: 0.5753 - val_loss: 1.2420 - val_accuracy: 0.5556\n",
      "Epoch 164/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.0304 - accuracy: 0.5728 - val_loss: 1.2265 - val_accuracy: 0.5441\n",
      "Epoch 165/1000\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 1.0132 - accuracy: 0.5844 - val_loss: 1.2195 - val_accuracy: 0.5862\n",
      "Epoch 166/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 1.0023 - accuracy: 0.5802 - val_loss: 1.2264 - val_accuracy: 0.5536\n",
      "Epoch 167/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 1.0126 - accuracy: 0.5737 - val_loss: 1.2459 - val_accuracy: 0.5077\n",
      "Epoch 168/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.9591 - accuracy: 0.5926 - val_loss: 1.2415 - val_accuracy: 0.5517\n",
      "Epoch 169/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.9744 - accuracy: 0.5893 - val_loss: 1.2400 - val_accuracy: 0.5556\n",
      "Epoch 170/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.9802 - accuracy: 0.5901 - val_loss: 1.2125 - val_accuracy: 0.5651\n",
      "Epoch 171/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.9986 - accuracy: 0.5860 - val_loss: 1.2370 - val_accuracy: 0.5728\n",
      "Epoch 172/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.9966 - accuracy: 0.5695 - val_loss: 1.2548 - val_accuracy: 0.5479\n",
      "Epoch 173/1000\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 1.0015 - accuracy: 0.5720 - val_loss: 1.2497 - val_accuracy: 0.5556\n",
      "Epoch 174/1000\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 0.9670 - accuracy: 0.5877 - val_loss: 1.2200 - val_accuracy: 0.5613\n",
      "Epoch 175/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.9572 - accuracy: 0.6099 - val_loss: 1.2546 - val_accuracy: 0.5613\n",
      "Epoch 176/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.9871 - accuracy: 0.5827 - val_loss: 1.2441 - val_accuracy: 0.5536\n",
      "Epoch 177/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.9680 - accuracy: 0.5959 - val_loss: 1.2285 - val_accuracy: 0.5785\n",
      "Epoch 178/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.9736 - accuracy: 0.6049 - val_loss: 1.2644 - val_accuracy: 0.5613\n",
      "Epoch 179/1000\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 0.9641 - accuracy: 0.6016 - val_loss: 1.3206 - val_accuracy: 0.5498\n",
      "Epoch 180/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.9605 - accuracy: 0.6091 - val_loss: 1.2341 - val_accuracy: 0.5747\n",
      "Epoch 181/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.9496 - accuracy: 0.6033 - val_loss: 1.2518 - val_accuracy: 0.5747\n",
      "Epoch 182/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.9524 - accuracy: 0.6148 - val_loss: 1.2432 - val_accuracy: 0.5441\n",
      "Epoch 183/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.9716 - accuracy: 0.5835 - val_loss: 1.2157 - val_accuracy: 0.5651\n",
      "Epoch 184/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.9385 - accuracy: 0.6156 - val_loss: 1.2059 - val_accuracy: 0.5785\n",
      "Epoch 185/1000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.9583 - accuracy: 0.6140 - val_loss: 1.1834 - val_accuracy: 0.5728\n",
      "Epoch 186/1000\n",
      "38/38 [==============================] - 4s 114ms/step - loss: 0.9726 - accuracy: 0.5918 - val_loss: 1.1975 - val_accuracy: 0.5709\n",
      "Epoch 187/1000\n",
      "38/38 [==============================] - 3s 88ms/step - loss: 0.9400 - accuracy: 0.5909 - val_loss: 1.2023 - val_accuracy: 0.5613\n",
      "Epoch 188/1000\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 0.9652 - accuracy: 0.5992 - val_loss: 1.2433 - val_accuracy: 0.5632\n",
      "Epoch 189/1000\n",
      "38/38 [==============================] - 4s 93ms/step - loss: 0.9928 - accuracy: 0.5852 - val_loss: 1.2802 - val_accuracy: 0.5383\n",
      "Epoch 190/1000\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 0.9612 - accuracy: 0.5819 - val_loss: 1.1950 - val_accuracy: 0.5766\n",
      "Epoch 191/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.9697 - accuracy: 0.5901 - val_loss: 1.2398 - val_accuracy: 0.5441\n",
      "Epoch 192/1000\n",
      "38/38 [==============================] - 1s 33ms/step - loss: 0.9453 - accuracy: 0.6033 - val_loss: 1.2272 - val_accuracy: 0.5709\n",
      "Epoch 193/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.9606 - accuracy: 0.6173 - val_loss: 1.2493 - val_accuracy: 0.5805\n",
      "Epoch 194/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.9383 - accuracy: 0.6049 - val_loss: 1.1945 - val_accuracy: 0.5632\n",
      "Epoch 195/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 0.9598 - accuracy: 0.5959 - val_loss: 1.2030 - val_accuracy: 0.5843\n",
      "Epoch 196/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 0.9407 - accuracy: 0.6049 - val_loss: 1.2183 - val_accuracy: 0.5900\n",
      "Epoch 197/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.9099 - accuracy: 0.6165 - val_loss: 1.1710 - val_accuracy: 0.5958\n",
      "Epoch 198/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.9545 - accuracy: 0.6058 - val_loss: 1.2426 - val_accuracy: 0.5498\n",
      "Epoch 199/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.9410 - accuracy: 0.5959 - val_loss: 1.2218 - val_accuracy: 0.5556\n",
      "Epoch 200/1000\n",
      "38/38 [==============================] - 1s 33ms/step - loss: 0.9250 - accuracy: 0.6058 - val_loss: 1.2301 - val_accuracy: 0.5651\n",
      "Epoch 201/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.9173 - accuracy: 0.6247 - val_loss: 1.2187 - val_accuracy: 0.5843\n",
      "Epoch 202/1000\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 0.9334 - accuracy: 0.6099 - val_loss: 1.2843 - val_accuracy: 0.5421\n",
      "Epoch 203/1000\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 0.9277 - accuracy: 0.6140 - val_loss: 1.2395 - val_accuracy: 0.5613\n",
      "Epoch 204/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.9284 - accuracy: 0.6082 - val_loss: 1.1955 - val_accuracy: 0.5728\n",
      "Epoch 205/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 0.9155 - accuracy: 0.6189 - val_loss: 1.2985 - val_accuracy: 0.5383\n",
      "Epoch 206/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.9458 - accuracy: 0.6058 - val_loss: 1.1895 - val_accuracy: 0.5785\n",
      "Epoch 207/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.8789 - accuracy: 0.6362 - val_loss: 1.1923 - val_accuracy: 0.5900\n",
      "Epoch 208/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.9195 - accuracy: 0.6132 - val_loss: 1.2108 - val_accuracy: 0.5594\n",
      "Epoch 209/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.9222 - accuracy: 0.6189 - val_loss: 1.2136 - val_accuracy: 0.5747\n",
      "Epoch 210/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.8959 - accuracy: 0.6255 - val_loss: 1.1964 - val_accuracy: 0.5824\n",
      "Epoch 211/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.9292 - accuracy: 0.6016 - val_loss: 1.1987 - val_accuracy: 0.5575\n",
      "Epoch 212/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.9126 - accuracy: 0.6025 - val_loss: 1.2278 - val_accuracy: 0.5709\n",
      "Epoch 213/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.9079 - accuracy: 0.6370 - val_loss: 1.2206 - val_accuracy: 0.5747\n",
      "Epoch 214/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.8949 - accuracy: 0.6091 - val_loss: 1.1984 - val_accuracy: 0.5766\n",
      "Epoch 215/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8658 - accuracy: 0.6403 - val_loss: 1.2174 - val_accuracy: 0.5805\n",
      "Epoch 216/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.8831 - accuracy: 0.6239 - val_loss: 1.2579 - val_accuracy: 0.5402\n",
      "Epoch 217/1000\n",
      "38/38 [==============================] - 2s 41ms/step - loss: 0.9174 - accuracy: 0.6272 - val_loss: 1.2453 - val_accuracy: 0.5824\n",
      "Epoch 218/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.9270 - accuracy: 0.6041 - val_loss: 1.2090 - val_accuracy: 0.5632\n",
      "Epoch 219/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 1s 38ms/step - loss: 0.8975 - accuracy: 0.6255 - val_loss: 1.1933 - val_accuracy: 0.6054\n",
      "Epoch 220/1000\n",
      "38/38 [==============================] - 2s 47ms/step - loss: 0.8827 - accuracy: 0.6362 - val_loss: 1.2186 - val_accuracy: 0.5747\n",
      "Epoch 221/1000\n",
      "38/38 [==============================] - 4s 109ms/step - loss: 0.8770 - accuracy: 0.6370 - val_loss: 1.2194 - val_accuracy: 0.5690\n",
      "Epoch 222/1000\n",
      "38/38 [==============================] - 4s 108ms/step - loss: 0.8879 - accuracy: 0.6354 - val_loss: 1.2194 - val_accuracy: 0.5977\n",
      "Epoch 223/1000\n",
      "38/38 [==============================] - 5s 119ms/step - loss: 0.8923 - accuracy: 0.6387 - val_loss: 1.2297 - val_accuracy: 0.5728\n",
      "Epoch 224/1000\n",
      "38/38 [==============================] - 3s 79ms/step - loss: 0.8979 - accuracy: 0.6272 - val_loss: 1.2010 - val_accuracy: 0.5690\n",
      "Epoch 225/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 0.8750 - accuracy: 0.6263 - val_loss: 1.2033 - val_accuracy: 0.5958\n",
      "Epoch 226/1000\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.8973 - accuracy: 0.6156 - val_loss: 1.1807 - val_accuracy: 0.5881\n",
      "Epoch 227/1000\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 0.8932 - accuracy: 0.6156 - val_loss: 1.2052 - val_accuracy: 0.6034\n",
      "Epoch 228/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.8771 - accuracy: 0.6346 - val_loss: 1.2351 - val_accuracy: 0.5843\n",
      "Epoch 229/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.9087 - accuracy: 0.6239 - val_loss: 1.1978 - val_accuracy: 0.5594\n",
      "Epoch 230/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.8660 - accuracy: 0.6280 - val_loss: 1.2104 - val_accuracy: 0.5805\n",
      "Epoch 231/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.8682 - accuracy: 0.6247 - val_loss: 1.1774 - val_accuracy: 0.5881\n",
      "Epoch 232/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 0.8650 - accuracy: 0.6362 - val_loss: 1.1920 - val_accuracy: 0.6130\n",
      "Epoch 233/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.9083 - accuracy: 0.6313 - val_loss: 1.2133 - val_accuracy: 0.5785\n",
      "Epoch 234/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.8709 - accuracy: 0.6181 - val_loss: 1.1856 - val_accuracy: 0.5824\n",
      "Epoch 235/1000\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 0.9061 - accuracy: 0.6033 - val_loss: 1.2361 - val_accuracy: 0.5843\n",
      "Epoch 236/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8646 - accuracy: 0.6370 - val_loss: 1.2156 - val_accuracy: 0.5556\n",
      "Epoch 237/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8579 - accuracy: 0.6321 - val_loss: 1.1983 - val_accuracy: 0.5824\n",
      "Epoch 238/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8866 - accuracy: 0.6280 - val_loss: 1.2180 - val_accuracy: 0.5690\n",
      "Epoch 239/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8685 - accuracy: 0.6354 - val_loss: 1.2503 - val_accuracy: 0.5575\n",
      "Epoch 240/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8477 - accuracy: 0.6436 - val_loss: 1.1771 - val_accuracy: 0.6092\n",
      "Epoch 241/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.8720 - accuracy: 0.6247 - val_loss: 1.2104 - val_accuracy: 0.5670\n",
      "Epoch 242/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.8717 - accuracy: 0.6420 - val_loss: 1.1836 - val_accuracy: 0.6245\n",
      "Epoch 243/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 0.8613 - accuracy: 0.6288 - val_loss: 1.2509 - val_accuracy: 0.5920\n",
      "Epoch 244/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.8667 - accuracy: 0.6370 - val_loss: 1.2103 - val_accuracy: 0.5785\n",
      "Epoch 245/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8409 - accuracy: 0.6494 - val_loss: 1.2403 - val_accuracy: 0.5651\n",
      "Epoch 246/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.8555 - accuracy: 0.6477 - val_loss: 1.2451 - val_accuracy: 0.6034\n",
      "Epoch 247/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8745 - accuracy: 0.6247 - val_loss: 1.2194 - val_accuracy: 0.5785\n",
      "Epoch 248/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.8323 - accuracy: 0.6362 - val_loss: 1.2419 - val_accuracy: 0.6034\n",
      "Epoch 249/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8488 - accuracy: 0.6379 - val_loss: 1.2185 - val_accuracy: 0.5900\n",
      "Epoch 250/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.8748 - accuracy: 0.6305 - val_loss: 1.1968 - val_accuracy: 0.6015\n",
      "Epoch 251/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8556 - accuracy: 0.6329 - val_loss: 1.1659 - val_accuracy: 0.6054\n",
      "Epoch 252/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8362 - accuracy: 0.6576 - val_loss: 1.1999 - val_accuracy: 0.6054\n",
      "Epoch 253/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8444 - accuracy: 0.6403 - val_loss: 1.1905 - val_accuracy: 0.5996\n",
      "Epoch 254/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8179 - accuracy: 0.6494 - val_loss: 1.2465 - val_accuracy: 0.5958\n",
      "Epoch 255/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8491 - accuracy: 0.6510 - val_loss: 1.2265 - val_accuracy: 0.5843\n",
      "Epoch 256/1000\n",
      "38/38 [==============================] - 3s 80ms/step - loss: 0.9059 - accuracy: 0.6305 - val_loss: 1.1837 - val_accuracy: 0.5977\n",
      "Epoch 257/1000\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 0.8496 - accuracy: 0.6412 - val_loss: 1.1859 - val_accuracy: 0.5881\n",
      "Epoch 258/1000\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.8367 - accuracy: 0.6379 - val_loss: 1.2009 - val_accuracy: 0.6130\n",
      "Epoch 259/1000\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.8341 - accuracy: 0.6461 - val_loss: 1.2062 - val_accuracy: 0.5805\n",
      "Epoch 260/1000\n",
      "38/38 [==============================] - 3s 79ms/step - loss: 0.8519 - accuracy: 0.6379 - val_loss: 1.2824 - val_accuracy: 0.5690\n",
      "Epoch 261/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.8310 - accuracy: 0.6568 - val_loss: 1.2163 - val_accuracy: 0.5805\n",
      "Epoch 262/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8705 - accuracy: 0.6346 - val_loss: 1.2696 - val_accuracy: 0.5747\n",
      "Epoch 263/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8795 - accuracy: 0.6214 - val_loss: 1.2437 - val_accuracy: 0.5900\n",
      "Epoch 264/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8289 - accuracy: 0.6494 - val_loss: 1.2222 - val_accuracy: 0.5920\n",
      "Epoch 265/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8548 - accuracy: 0.6494 - val_loss: 1.2161 - val_accuracy: 0.5939\n",
      "Epoch 266/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8381 - accuracy: 0.6486 - val_loss: 1.1846 - val_accuracy: 0.6149\n",
      "Epoch 267/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.8122 - accuracy: 0.6609 - val_loss: 1.2365 - val_accuracy: 0.6092\n",
      "Epoch 268/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.8396 - accuracy: 0.6395 - val_loss: 1.2846 - val_accuracy: 0.6073\n",
      "Epoch 269/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8182 - accuracy: 0.6560 - val_loss: 1.2851 - val_accuracy: 0.6015\n",
      "Epoch 270/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8192 - accuracy: 0.6535 - val_loss: 1.2270 - val_accuracy: 0.5900\n",
      "Epoch 271/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8289 - accuracy: 0.6420 - val_loss: 1.2611 - val_accuracy: 0.6226\n",
      "Epoch 272/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8135 - accuracy: 0.6601 - val_loss: 1.2795 - val_accuracy: 0.5766\n",
      "Epoch 273/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8595 - accuracy: 0.6395 - val_loss: 1.2891 - val_accuracy: 0.5805\n",
      "Epoch 274/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 0.8147 - accuracy: 0.6724 - val_loss: 1.2620 - val_accuracy: 0.6111\n",
      "Epoch 275/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.8085 - accuracy: 0.6519 - val_loss: 1.2263 - val_accuracy: 0.6015\n",
      "Epoch 276/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.8309 - accuracy: 0.6494 - val_loss: 1.2252 - val_accuracy: 0.5920\n",
      "Epoch 277/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.8286 - accuracy: 0.6403 - val_loss: 1.2703 - val_accuracy: 0.5632\n",
      "Epoch 278/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.8438 - accuracy: 0.6362 - val_loss: 1.2111 - val_accuracy: 0.6073\n",
      "Epoch 279/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8163 - accuracy: 0.6576 - val_loss: 1.2538 - val_accuracy: 0.6015\n",
      "Epoch 280/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.8339 - accuracy: 0.6560 - val_loss: 1.2512 - val_accuracy: 0.5939\n",
      "Epoch 281/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8290 - accuracy: 0.6477 - val_loss: 1.2405 - val_accuracy: 0.5881\n",
      "Epoch 282/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.8358 - accuracy: 0.6510 - val_loss: 1.2704 - val_accuracy: 0.5651\n",
      "Epoch 283/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8177 - accuracy: 0.6519 - val_loss: 1.2122 - val_accuracy: 0.5958\n",
      "Epoch 284/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.8122 - accuracy: 0.6510 - val_loss: 1.2605 - val_accuracy: 0.5747\n",
      "Epoch 285/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8199 - accuracy: 0.6486 - val_loss: 1.2512 - val_accuracy: 0.6149\n",
      "Epoch 286/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8105 - accuracy: 0.6576 - val_loss: 1.2188 - val_accuracy: 0.6245\n",
      "Epoch 287/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.7880 - accuracy: 0.6626 - val_loss: 1.3096 - val_accuracy: 0.5843\n",
      "Epoch 288/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 0.8253 - accuracy: 0.6576 - val_loss: 1.2674 - val_accuracy: 0.6188\n",
      "Epoch 289/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 0.7991 - accuracy: 0.6568 - val_loss: 1.2404 - val_accuracy: 0.6054\n",
      "Epoch 290/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.7936 - accuracy: 0.6634 - val_loss: 1.2423 - val_accuracy: 0.6130\n",
      "Epoch 291/1000\n",
      "38/38 [==============================] - 3s 91ms/step - loss: 0.7945 - accuracy: 0.6543 - val_loss: 1.2750 - val_accuracy: 0.6034\n",
      "Epoch 292/1000\n",
      "38/38 [==============================] - 4s 118ms/step - loss: 0.8217 - accuracy: 0.6461 - val_loss: 1.2304 - val_accuracy: 0.6092\n",
      "Epoch 293/1000\n",
      "38/38 [==============================] - 4s 102ms/step - loss: 0.8030 - accuracy: 0.6617 - val_loss: 1.2410 - val_accuracy: 0.6111\n",
      "Epoch 294/1000\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 0.8204 - accuracy: 0.6576 - val_loss: 1.2987 - val_accuracy: 0.5709\n",
      "Epoch 295/1000\n",
      "38/38 [==============================] - 2s 47ms/step - loss: 0.8318 - accuracy: 0.6362 - val_loss: 1.2765 - val_accuracy: 0.5766\n",
      "Epoch 296/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.8267 - accuracy: 0.6420 - val_loss: 1.2606 - val_accuracy: 0.6015\n",
      "Epoch 297/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8095 - accuracy: 0.6617 - val_loss: 1.2833 - val_accuracy: 0.5900\n",
      "Epoch 298/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8045 - accuracy: 0.6667 - val_loss: 1.2069 - val_accuracy: 0.6398\n",
      "Epoch 299/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.8091 - accuracy: 0.6444 - val_loss: 1.2112 - val_accuracy: 0.6034\n",
      "Epoch 300/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 0.8028 - accuracy: 0.6626 - val_loss: 1.2234 - val_accuracy: 0.6111\n",
      "Epoch 301/1000\n",
      "38/38 [==============================] - 2s 40ms/step - loss: 0.7981 - accuracy: 0.6749 - val_loss: 1.2077 - val_accuracy: 0.6207\n",
      "Epoch 302/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.7891 - accuracy: 0.6626 - val_loss: 1.2448 - val_accuracy: 0.5881\n",
      "Epoch 303/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.7988 - accuracy: 0.6543 - val_loss: 1.2783 - val_accuracy: 0.5996\n",
      "Epoch 304/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 0.7971 - accuracy: 0.6527 - val_loss: 1.2485 - val_accuracy: 0.6264\n",
      "Epoch 305/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.7934 - accuracy: 0.6650 - val_loss: 1.2263 - val_accuracy: 0.6149\n",
      "Epoch 306/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.7813 - accuracy: 0.6716 - val_loss: 1.3060 - val_accuracy: 0.5939\n",
      "Epoch 307/1000\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 0.8110 - accuracy: 0.6617 - val_loss: 1.2526 - val_accuracy: 0.5996\n",
      "Epoch 308/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.7887 - accuracy: 0.6658 - val_loss: 1.2693 - val_accuracy: 0.5881\n",
      "Epoch 309/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.7991 - accuracy: 0.6749 - val_loss: 1.2614 - val_accuracy: 0.6073\n",
      "Epoch 310/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.7716 - accuracy: 0.6733 - val_loss: 1.2964 - val_accuracy: 0.5920\n",
      "Epoch 311/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 0.8059 - accuracy: 0.6576 - val_loss: 1.2691 - val_accuracy: 0.5766\n",
      "Epoch 312/1000\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 0.8201 - accuracy: 0.6337 - val_loss: 1.2712 - val_accuracy: 0.5977\n",
      "Epoch 313/1000\n",
      "38/38 [==============================] - 1s 38ms/step - loss: 0.7745 - accuracy: 0.6675 - val_loss: 1.2468 - val_accuracy: 0.6073\n",
      "Epoch 314/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.7899 - accuracy: 0.6551 - val_loss: 1.2350 - val_accuracy: 0.6207\n",
      "Epoch 315/1000\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 0.8136 - accuracy: 0.6494 - val_loss: 1.3031 - val_accuracy: 0.5881\n",
      "Epoch 316/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.7614 - accuracy: 0.6848 - val_loss: 1.2412 - val_accuracy: 0.6360\n",
      "Epoch 317/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.8030 - accuracy: 0.6510 - val_loss: 1.2350 - val_accuracy: 0.6073\n",
      "Epoch 318/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.7831 - accuracy: 0.6576 - val_loss: 1.2431 - val_accuracy: 0.6073\n",
      "Epoch 319/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.7870 - accuracy: 0.6700 - val_loss: 1.2667 - val_accuracy: 0.6054\n",
      "Epoch 320/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.7764 - accuracy: 0.6700 - val_loss: 1.2237 - val_accuracy: 0.6092\n",
      "Epoch 321/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.7716 - accuracy: 0.6642 - val_loss: 1.2303 - val_accuracy: 0.6073\n",
      "Epoch 322/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.7716 - accuracy: 0.6642 - val_loss: 1.3363 - val_accuracy: 0.5881\n",
      "Epoch 323/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.7783 - accuracy: 0.6601 - val_loss: 1.2119 - val_accuracy: 0.6130\n",
      "Epoch 324/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.7872 - accuracy: 0.6675 - val_loss: 1.2256 - val_accuracy: 0.6130\n",
      "Epoch 325/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.7774 - accuracy: 0.6765 - val_loss: 1.3077 - val_accuracy: 0.5594\n",
      "Epoch 326/1000\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.7710 - accuracy: 0.6749 - val_loss: 1.2657 - val_accuracy: 0.6169\n",
      "Epoch 327/1000\n",
      "38/38 [==============================] - 3s 91ms/step - loss: 0.7575 - accuracy: 0.6749 - val_loss: 1.2670 - val_accuracy: 0.6245\n",
      "Epoch 328/1000\n",
      "38/38 [==============================] - 4s 100ms/step - loss: 0.7873 - accuracy: 0.6609 - val_loss: 1.3344 - val_accuracy: 0.6054\n",
      "Epoch 329/1000\n",
      "38/38 [==============================] - 3s 86ms/step - loss: 0.7985 - accuracy: 0.6733 - val_loss: 1.2898 - val_accuracy: 0.5996\n",
      "Epoch 330/1000\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.7795 - accuracy: 0.6798 - val_loss: 1.2858 - val_accuracy: 0.5996\n",
      "Epoch 331/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 2s 57ms/step - loss: 0.7569 - accuracy: 0.6741 - val_loss: 1.2542 - val_accuracy: 0.6456\n",
      "Epoch 332/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.7976 - accuracy: 0.6650 - val_loss: 1.2216 - val_accuracy: 0.6245\n",
      "Epoch 333/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.7627 - accuracy: 0.6642 - val_loss: 1.2549 - val_accuracy: 0.5977\n",
      "Epoch 334/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.7383 - accuracy: 0.6724 - val_loss: 1.2573 - val_accuracy: 0.6226\n",
      "Epoch 335/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.7911 - accuracy: 0.6560 - val_loss: 1.3300 - val_accuracy: 0.5728\n",
      "Epoch 336/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.7552 - accuracy: 0.6683 - val_loss: 1.2894 - val_accuracy: 0.6111\n",
      "Epoch 337/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.7837 - accuracy: 0.6708 - val_loss: 1.3041 - val_accuracy: 0.5958\n",
      "Epoch 338/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.7804 - accuracy: 0.6650 - val_loss: 1.2878 - val_accuracy: 0.5824\n",
      "Epoch 339/1000\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 0.7608 - accuracy: 0.6774 - val_loss: 1.2711 - val_accuracy: 0.5862\n",
      "Epoch 340/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.7631 - accuracy: 0.6733 - val_loss: 1.2505 - val_accuracy: 0.6226\n",
      "Epoch 341/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.7487 - accuracy: 0.6675 - val_loss: 1.2879 - val_accuracy: 0.5900\n",
      "Epoch 342/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.7629 - accuracy: 0.6782 - val_loss: 1.2361 - val_accuracy: 0.6073\n",
      "Epoch 343/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.7788 - accuracy: 0.6658 - val_loss: 1.2899 - val_accuracy: 0.6149\n",
      "Epoch 344/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.7675 - accuracy: 0.6634 - val_loss: 1.2440 - val_accuracy: 0.5843\n",
      "Epoch 345/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.7846 - accuracy: 0.6601 - val_loss: 1.2380 - val_accuracy: 0.5862\n",
      "Epoch 346/1000\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.7747 - accuracy: 0.6584 - val_loss: 1.2826 - val_accuracy: 0.6149\n",
      "Epoch 347/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.7888 - accuracy: 0.6634 - val_loss: 1.2206 - val_accuracy: 0.6379\n",
      "Epoch 348/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.7307 - accuracy: 0.6897 - val_loss: 1.2334 - val_accuracy: 0.6456\n",
      "Epoch 349/1000\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.7458 - accuracy: 0.6782 - val_loss: 1.2157 - val_accuracy: 0.6130\n",
      "Epoch 350/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.7320 - accuracy: 0.6979 - val_loss: 1.3067 - val_accuracy: 0.5651\n",
      "Epoch 351/1000\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.7757 - accuracy: 0.6807 - val_loss: 1.2217 - val_accuracy: 0.6379\n",
      "Validation Accuracy: 60.536%\n"
     ]
    }
   ],
   "source": [
    "# # Training on validation data (Experimental)\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# y_val = Data_val.Emotion\n",
    "# X_val = pd.DataFrame(Data_val.drop(['Emotion', 'User'], axis = 1))\n",
    "\n",
    "# X_train_val, X_test_val, y_train_val, y_test_val = train_test_split(X_val, y_val, test_size = 0.3, random_state = 123)\n",
    "\n",
    "\n",
    "# #Trying adam optimizer\n",
    "# modelVal = initModelGRU(X.shape[1], 7, 'softmax')\n",
    "# modelVal.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss='sparse_categorical_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# historyVal = modelVal.fit(\n",
    "#     X_train_val,\n",
    "#     y_train_val,\n",
    "#     validation_data = (X_test_val, y_test_val),\n",
    "#     epochs=1000,\n",
    "#     callbacks=[\n",
    "#         tf.keras.callbacks.EarlyStopping(\n",
    "#             monitor='val_loss',\n",
    "#             patience=100,\n",
    "#             restore_best_weights=True\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# #Result of adam optimizer on validation data\n",
    "# model_acc = modelVal.evaluate(X_test_val, y_test_val, verbose=0)[1]\n",
    "# print(\"Validation Accuracy: {:.3f}%\".format(model_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2cd3b7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Plot the accuracy curve for validation set\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(historyVal.history['val_accuracy'], color='r', label=\"Accuracy\")\n",
    "\n",
    "# plt.title(\"Val accuracy\")\n",
    "# plt.xlabel(\"Number of Epochs\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b225588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict = model.predict(X_test)[0]\n",
    "# predict = [np.argmax(pre) for pre in predict]\n",
    "# print(predict)\n",
    "# y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e5e15a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
